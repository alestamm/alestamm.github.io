{"status":"ok","feed":{"url":"https://medium.com/feed/@alestamm","title":"Stories by Alexandre Stamm on Medium","link":"https://medium.com/@alestamm?source=rss-bdc9fb848ad5------2","author":"","description":"Stories by Alexandre Stamm on Medium","image":"https://cdn-images-1.medium.com/fit/c/150/150/1*VB-2ppzDDRBVB2kyIPlesA.jpeg"},"items":[{"title":"Automating reports with Google Sheets API &amp; Jupyter Notebook connection","pubDate":"2022-11-26 16:42:43","link":"https://medium.com/@alestamm/automating-reports-with-google-sheets-jupyter-notebook-connection-8f9cfa5e8588?source=rss-bdc9fb848ad5------2","guid":"https://medium.com/p/8f9cfa5e8588","author":"Alexandre Stamm","thumbnail":"https://cdn-images-1.medium.com/max/1024/0*iPchLpOYBd6tqbSD","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*iPchLpOYBd6tqbSD\"><figcaption>Photo by <a href=\"https://unsplash.com/@homajob?utm_source=medium&amp;utm_medium=referral\">Scott Graham</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>Hello everyone!</p>\n<p>Today I\u2019m going to talk about how we can integrate a Jupyter Notebook with the Google Sheets API so we can make imports &amp; exports easier, and maybe automate a report that we need to update on a weekly basis, for\u00a0example.</p>\n<h3>Setting up Google API Credentials</h3>\n<p>To make this connection work, we first need to setup our Google API Credentials. Here\u2019s a step-by-step guide on how to do\u00a0this:</p>\n<ul>\n<li>If you don\u2019t have a Google cloud account yet, go to <a href=\"https://console.cloud.google.com/\">Google Cloud</a> and create\u00a0one</li>\n<li>In the Cloud Console, on the project list, select a existing project or create a new one following these\u00a0steps:</li>\n</ul>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*jY0qhBxfQBLOFvwP7t5AXQ.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/755/1*rce770k8loIHg6BkOz1E0A.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/586/1*ZA6G32kfxbZ58NSakyGpAw.png\"></figure><p>After you create your\u00a0project:</p>\n<ol>\n<li>Navigate to <a href=\"https://console.cloud.google.com/apis/library\">Google API\u00a0Service</a>\n</li>\n<li>Search for the Google Sheets API in the search bar, and enable\u00a0it.</li>\n<li>Select Create Credentials (located on the top right of the\u00a0screen)</li>\n<li>Follow the steps for creating the credentials, selecting application data as the data will you be accessing</li>\n<li>For the \u201cAre you planning to use this API with Compute Engine, Kubernetes Engine, App Engine, or Cloud Functions?\u201d Select \u201cNo, I\u2019m not using\u00a0them\u201d</li>\n<li>Name the Service Account, and assign it the Owner role for your project, then click\u00a0<strong>Done.</strong>\n</li>\n</ol>\n<a href=\"https://medium.com/media/eb34e7c36e175e0737176673f9f7695d/href\">https://medium.com/media/eb34e7c36e175e0737176673f9f7695d/href</a><p>From the <strong>Service Accounts</strong> page, click on the Service Account you just created, and go to the Keys\u00a0tab:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/857/1*aOR6_LuP_3A0isQ_eTei4Q.png\"></figure><ol>\n<li>Click <strong>Add Key</strong> &gt; <strong>Create new\u00a0key</strong>\n</li>\n<li>Choose JSON, then click <strong>Create</strong>. The JSON file will download automatically.</li>\n</ol>\n<h3>Connecting our Jupyter Notebook to Google\u00a0Sheets:</h3>\n<p>Now that we have our Service Account and JSON with our account information, we can load this into our notebook. To do this, we have to follow some\u00a0steps:</p>\n<ol><li>Install <a href=\"https://docs.gspread.org/en/v5.7.0/\">gspread</a>, that is a Python API for Google\u00a0Sheets.</li></ol>\n<pre>!pip install gspread</pre>\n<p>2. Now, we import the packages that we will\u00a0use:</p>\n<pre>import pandas as pd<br>import gspread<br>from google.oauth2 import service_account<br>import json<br>import seaborn as sns</pre>\n<p>3. Import credentials information from our json file (the file should be in the same directory as your Jupyter Notebook):</p>\n<pre>credentials = service_account.Credentials.from_service_account_file(<br>    'google_credentials.json')</pre>\n<p>One important thing to keep in mind is that by importing our credential data like this, the information will be<strong> saved into a variable</strong>. That means if someone access your notebook, that person can visualize your credentials information (which is not good security-wise). There are a number of ways to avoid a possible \u201cleak\u201d, one of them is using a\u00a0.env file, which I explained as a part of <a href=\"https://medium.com/@alestamm/importing-data-from-a-postgresql-database-to-a-pandas-dataframe-5f4bffcd8bb2\">this</a>\u00a0guide.</p>\n<p>4. Define an API scope, assign it to our credentials, and authorize a new client with the credentials:</p>\n<pre>scope = ['https://spreadsheets.google.com/feeds']<br>creds_scope = credentials.with_scopes(scope)<br>client = gspread.authorize(creds_scope)</pre>\n<p>5. To be able to access our spreadsheet from our Notebook, we have to share it with our Service Account. You can do this by copying the client_email link that is inside our json file and pasting it on the <strong>\u201cShare\u201d</strong> button in our Google Sheet. Remember to assign it the <strong>\u201cEditor\u201d</strong> permission.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*nS7iGVx4gluWAP8v7KLIvA.png\"></figure><p>6. Now, we can create a connection using the link of our Google\u00a0Sheet:</p>\n<pre>sheet = client.open_by_url('paste_your_google_sheet_link_here')</pre>\n<p>7. Then we create a new variable with our worksheet, using it\u2019s name or\u00a0index:</p>\n<pre># using the worksheet name<br>sheet_export = sheet.worksheet(\"export_example\")<br><br># using the worksheet index (starting with 0)<br>sheet_export = sh.get_worksheet(0)</pre>\n<h3>Exporting information from pandas to Google\u00a0Sheets:</h3>\n<p>Let\u2019s import a sample dataset to test how exporting information from our Notebook to Google Sheets\u00a0works:</p>\n<pre>penguins = sns.load_dataset('penguins')<br>penguins.info()</pre>\n<pre>Data columns (total 7 columns):<br> #   Column             Non-Null Count  Dtype  <br>---  ------             --------------  -----  <br> 0   species            344 non-null    object <br> 1   island             344 non-null    object <br> 2   bill_length_mm     342 non-null    float64<br> 3   bill_depth_mm      342 non-null    float64<br> 4   flipper_length_mm  342 non-null    float64<br> 5   body_mass_g        342 non-null    float64<br> 6   sex                333 non-null    object</pre>\n<p>We can see that there are some nulls in our dataset. Trying to export a dataframe with null values sometimes results in an error, so, for the sake of this example, let\u2019s just drop\u00a0them:</p>\n<pre>penguins.dropna(inplace=True)</pre>\n<p>Now, if we just and to insert the values in our worksheet, we can use <a href=\"https://docs.gspread.org/en/v3.7.0/api.html#gspread.models.Worksheet.insert_rows\">insert_rows</a>, that takes a list and an index as arguments, and adds multiple rows to the worksheet at the specified index and populates them with\u00a0values.</p>\n<pre>sheet_export.insert_rows(penguins.values.tolist())<br>sheet_export.insert_rows([penguins.columns.tolist()])</pre>\n<p>Since the default index is 0, it inserts the newest values at the top (that\u2019s why we\u2019re adding the column names after the\u00a0values).</p>\n<p>If we want to replace the entire worksheet with new values, we can use\u00a0<a href=\"https://docs.gspread.org/en/v3.7.0/api.html#gspread.models.Worksheet.update\">update</a>:</p>\n<pre>sheet_export.update(<br>    [penguins.columns.tolist()]<br>    + penguins.values.tolist()<br>)</pre>\n<p>Using\u00a0.update(), the values go in the order presented to the function. That\u2019s why, in this case, we add the column names\u00a0first.</p>\n<p>And that\u2019s how we do simple export operations using the gspread\u00a0API!</p>\n<h3>Importing Data from Google Sheets to Jupyter Notebook:</h3>\n<p>Now, let\u2019s try the other way around: importing information from Google Sheets to a Jupyter Notebook!</p>\n<p>First, let\u2019s set a new \u201csheet\u201d variable with our import\u00a0tab:</p>\n<pre>sheet_import = sheet.worksheet(\"import_example\")</pre>\n<p>Now, we will use the get_all_records() feature to extract data from the worksheet:</p>\n<pre>penguins_records = sheet_import.get_all_records()</pre>\n<p>This will create a dictionary with all the information in the worksheet, and we can use pandas to turn it into a DataFrame:</p>\n<pre>penguins_2 = pd.DataFrame.from_dict(penguins_records)<br>penguins_2.head()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/615/1*QyNVCaInAZ6urmHs1ceGDg.png\"></figure><p>And that\u2019s it for importing data from Google\u00a0Sheets!</p>\n<p>There are also plenty other features in the API. You can see them all in the <a href=\"https://docs.gspread.org/en/v5.7.0/\">gspread documentation</a>.</p>\n<h3>Automating your\u00a0reports</h3>\n<p>Now that we know how to import and export information from Google Sheets into our Jupyter Notebook, we can combine this with a connection to our database (if you want to learn how to do this, check my <a href=\"https://medium.com/@alestamm/importing-data-from-a-postgresql-database-to-a-pandas-dataframe-5f4bffcd8bb2\"><strong>Importing data from a PostgreSQL database to a Pandas DataFrame</strong></a><strong> </strong>guide) and then automate a report we have to update on a daily/weekly/monthly basis, for example. We can import the data, transform it using Python, and export it to our worksheet every time we run our notebook!</p>\n<h3>Conclusion</h3>\n<p>In this tutorial, we learned how to import and export data from Google Sheets into a Jupyter Notebook, by setting up our Google Cloud account and gspread Python\u00a0API!</p>\n<p>Thanks for\u00a0reading!</p>\n<h4><strong>Sources:</strong></h4>\n<ul>\n<li><a href=\"https://docs.gspread.org/en/latest/user-guide.html\">Examples of gspread Usage - gspread 5.7.0 documentation</a></li>\n<li><a href=\"https://docs.gspread.org/en/latest/oauth2.html\">Authentication - gspread 5.7.0 documentation</a></li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8f9cfa5e8588\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*iPchLpOYBd6tqbSD\"><figcaption>Photo by <a href=\"https://unsplash.com/@homajob?utm_source=medium&amp;utm_medium=referral\">Scott Graham</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>Hello everyone!</p>\n<p>Today I\u2019m going to talk about how we can integrate a Jupyter Notebook with the Google Sheets API so we can make imports &amp; exports easier, and maybe automate a report that we need to update on a weekly basis, for\u00a0example.</p>\n<h3>Setting up Google API Credentials</h3>\n<p>To make this connection work, we first need to setup our Google API Credentials. Here\u2019s a step-by-step guide on how to do\u00a0this:</p>\n<ul>\n<li>If you don\u2019t have a Google cloud account yet, go to <a href=\"https://console.cloud.google.com/\">Google Cloud</a> and create\u00a0one</li>\n<li>In the Cloud Console, on the project list, select a existing project or create a new one following these\u00a0steps:</li>\n</ul>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*jY0qhBxfQBLOFvwP7t5AXQ.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/755/1*rce770k8loIHg6BkOz1E0A.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/586/1*ZA6G32kfxbZ58NSakyGpAw.png\"></figure><p>After you create your\u00a0project:</p>\n<ol>\n<li>Navigate to <a href=\"https://console.cloud.google.com/apis/library\">Google API\u00a0Service</a>\n</li>\n<li>Search for the Google Sheets API in the search bar, and enable\u00a0it.</li>\n<li>Select Create Credentials (located on the top right of the\u00a0screen)</li>\n<li>Follow the steps for creating the credentials, selecting application data as the data will you be accessing</li>\n<li>For the \u201cAre you planning to use this API with Compute Engine, Kubernetes Engine, App Engine, or Cloud Functions?\u201d Select \u201cNo, I\u2019m not using\u00a0them\u201d</li>\n<li>Name the Service Account, and assign it the Owner role for your project, then click\u00a0<strong>Done.</strong>\n</li>\n</ol>\n<a href=\"https://medium.com/media/eb34e7c36e175e0737176673f9f7695d/href\">https://medium.com/media/eb34e7c36e175e0737176673f9f7695d/href</a><p>From the <strong>Service Accounts</strong> page, click on the Service Account you just created, and go to the Keys\u00a0tab:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/857/1*aOR6_LuP_3A0isQ_eTei4Q.png\"></figure><ol>\n<li>Click <strong>Add Key</strong> &gt; <strong>Create new\u00a0key</strong>\n</li>\n<li>Choose JSON, then click <strong>Create</strong>. The JSON file will download automatically.</li>\n</ol>\n<h3>Connecting our Jupyter Notebook to Google\u00a0Sheets:</h3>\n<p>Now that we have our Service Account and JSON with our account information, we can load this into our notebook. To do this, we have to follow some\u00a0steps:</p>\n<ol><li>Install <a href=\"https://docs.gspread.org/en/v5.7.0/\">gspread</a>, that is a Python API for Google\u00a0Sheets.</li></ol>\n<pre>!pip install gspread</pre>\n<p>2. Now, we import the packages that we will\u00a0use:</p>\n<pre>import pandas as pd<br>import gspread<br>from google.oauth2 import service_account<br>import json<br>import seaborn as sns</pre>\n<p>3. Import credentials information from our json file (the file should be in the same directory as your Jupyter Notebook):</p>\n<pre>credentials = service_account.Credentials.from_service_account_file(<br>    'google_credentials.json')</pre>\n<p>One important thing to keep in mind is that by importing our credential data like this, the information will be<strong> saved into a variable</strong>. That means if someone access your notebook, that person can visualize your credentials information (which is not good security-wise). There are a number of ways to avoid a possible \u201cleak\u201d, one of them is using a\u00a0.env file, which I explained as a part of <a href=\"https://medium.com/@alestamm/importing-data-from-a-postgresql-database-to-a-pandas-dataframe-5f4bffcd8bb2\">this</a>\u00a0guide.</p>\n<p>4. Define an API scope, assign it to our credentials, and authorize a new client with the credentials:</p>\n<pre>scope = ['https://spreadsheets.google.com/feeds']<br>creds_scope = credentials.with_scopes(scope)<br>client = gspread.authorize(creds_scope)</pre>\n<p>5. To be able to access our spreadsheet from our Notebook, we have to share it with our Service Account. You can do this by copying the client_email link that is inside our json file and pasting it on the <strong>\u201cShare\u201d</strong> button in our Google Sheet. Remember to assign it the <strong>\u201cEditor\u201d</strong> permission.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*nS7iGVx4gluWAP8v7KLIvA.png\"></figure><p>6. Now, we can create a connection using the link of our Google\u00a0Sheet:</p>\n<pre>sheet = client.open_by_url('paste_your_google_sheet_link_here')</pre>\n<p>7. Then we create a new variable with our worksheet, using it\u2019s name or\u00a0index:</p>\n<pre># using the worksheet name<br>sheet_export = sheet.worksheet(\"export_example\")<br><br># using the worksheet index (starting with 0)<br>sheet_export = sh.get_worksheet(0)</pre>\n<h3>Exporting information from pandas to Google\u00a0Sheets:</h3>\n<p>Let\u2019s import a sample dataset to test how exporting information from our Notebook to Google Sheets\u00a0works:</p>\n<pre>penguins = sns.load_dataset('penguins')<br>penguins.info()</pre>\n<pre>Data columns (total 7 columns):<br> #   Column             Non-Null Count  Dtype  <br>---  ------             --------------  -----  <br> 0   species            344 non-null    object <br> 1   island             344 non-null    object <br> 2   bill_length_mm     342 non-null    float64<br> 3   bill_depth_mm      342 non-null    float64<br> 4   flipper_length_mm  342 non-null    float64<br> 5   body_mass_g        342 non-null    float64<br> 6   sex                333 non-null    object</pre>\n<p>We can see that there are some nulls in our dataset. Trying to export a dataframe with null values sometimes results in an error, so, for the sake of this example, let\u2019s just drop\u00a0them:</p>\n<pre>penguins.dropna(inplace=True)</pre>\n<p>Now, if we just and to insert the values in our worksheet, we can use <a href=\"https://docs.gspread.org/en/v3.7.0/api.html#gspread.models.Worksheet.insert_rows\">insert_rows</a>, that takes a list and an index as arguments, and adds multiple rows to the worksheet at the specified index and populates them with\u00a0values.</p>\n<pre>sheet_export.insert_rows(penguins.values.tolist())<br>sheet_export.insert_rows([penguins.columns.tolist()])</pre>\n<p>Since the default index is 0, it inserts the newest values at the top (that\u2019s why we\u2019re adding the column names after the\u00a0values).</p>\n<p>If we want to replace the entire worksheet with new values, we can use\u00a0<a href=\"https://docs.gspread.org/en/v3.7.0/api.html#gspread.models.Worksheet.update\">update</a>:</p>\n<pre>sheet_export.update(<br>    [penguins.columns.tolist()]<br>    + penguins.values.tolist()<br>)</pre>\n<p>Using\u00a0.update(), the values go in the order presented to the function. That\u2019s why, in this case, we add the column names\u00a0first.</p>\n<p>And that\u2019s how we do simple export operations using the gspread\u00a0API!</p>\n<h3>Importing Data from Google Sheets to Jupyter Notebook:</h3>\n<p>Now, let\u2019s try the other way around: importing information from Google Sheets to a Jupyter Notebook!</p>\n<p>First, let\u2019s set a new \u201csheet\u201d variable with our import\u00a0tab:</p>\n<pre>sheet_import = sheet.worksheet(\"import_example\")</pre>\n<p>Now, we will use the get_all_records() feature to extract data from the worksheet:</p>\n<pre>penguins_records = sheet_import.get_all_records()</pre>\n<p>This will create a dictionary with all the information in the worksheet, and we can use pandas to turn it into a DataFrame:</p>\n<pre>penguins_2 = pd.DataFrame.from_dict(penguins_records)<br>penguins_2.head()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/615/1*QyNVCaInAZ6urmHs1ceGDg.png\"></figure><p>And that\u2019s it for importing data from Google\u00a0Sheets!</p>\n<p>There are also plenty other features in the API. You can see them all in the <a href=\"https://docs.gspread.org/en/v5.7.0/\">gspread documentation</a>.</p>\n<h3>Automating your\u00a0reports</h3>\n<p>Now that we know how to import and export information from Google Sheets into our Jupyter Notebook, we can combine this with a connection to our database (if you want to learn how to do this, check my <a href=\"https://medium.com/@alestamm/importing-data-from-a-postgresql-database-to-a-pandas-dataframe-5f4bffcd8bb2\"><strong>Importing data from a PostgreSQL database to a Pandas DataFrame</strong></a><strong> </strong>guide) and then automate a report we have to update on a daily/weekly/monthly basis, for example. We can import the data, transform it using Python, and export it to our worksheet every time we run our notebook!</p>\n<h3>Conclusion</h3>\n<p>In this tutorial, we learned how to import and export data from Google Sheets into a Jupyter Notebook, by setting up our Google Cloud account and gspread Python\u00a0API!</p>\n<p>Thanks for\u00a0reading!</p>\n<h4><strong>Sources:</strong></h4>\n<ul>\n<li><a href=\"https://docs.gspread.org/en/latest/user-guide.html\">Examples of gspread Usage - gspread 5.7.0 documentation</a></li>\n<li><a href=\"https://docs.gspread.org/en/latest/oauth2.html\">Authentication - gspread 5.7.0 documentation</a></li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8f9cfa5e8588\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["pandas","google-cloud-platform","data-science","data-visualization","data"]},{"title":"Useful SQL Window functions for Business/Data Analysts","pubDate":"2022-09-09 12:08:45","link":"https://medium.com/@alestamm/useful-sql-window-functions-for-business-data-analysts-32420009ec21?source=rss-bdc9fb848ad5------2","guid":"https://medium.com/p/32420009ec21","author":"Alexandre Stamm","thumbnail":"https://cdn-images-1.medium.com/max/1024/0*8VKYvZ_9ZnOLZbb9","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*8VKYvZ_9ZnOLZbb9\"><figcaption>Photo by <a href=\"https://unsplash.com/@casparrubin?utm_source=medium&amp;utm_medium=referral\">Caspar Camille Rubin</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>Today, I\u2019m going to talk about some of the functions that I often use when I\u2019m working with SQL to query information for my analysis and dashboards.</p>\n<p>When using SQL for data analysis, some functions are very useful for dealing with everyday business problems.</p>\n<p>First, some basic functions don\u2019t require much explanation, and most SQL practitioners know how to use them. These functions are aggregate/mathematical functions (like sum, max/min, average, count, ceil), string manipulation functions (like lower, concat), or date manipulation functions (like extract or date_trunc).</p>\n<p>I\u2019m not going to go into detail about these functions since they are very straightforward, and you can learn their syntax by reading the documentation:</p>\n<p><a href=\"https://www.w3schools.com/sql/sql_ref_mysql.asp\">My SQL</a>, <a href=\"https://www.w3schools.com/sql/sql_ref_sqlserver.asp\">SQL Server</a>, <a href=\"https://docs.snowflake.com/en/\">Snowflake</a>, or <a href=\"https://www.postgresql.org/docs/current/functions.html\">PostgreSQL functions</a>. For other DBMS functions, you can look for their documentation on\u00a0Google.</p>\n<p>There are other types of very useful functions called <strong>Window functions</strong>. I\u2019m going to talk about these functions and their syntax, and also give examples of how to use\u00a0them.</p>\n<h4>SELECT\u200a\u2014\u200aOVER\u00a0Clause:</h4>\n<p>Select\u200a\u2014\u200aOVER, according to <a href=\"https://docs.microsoft.com/pt-br/sql/t-sql/queries/select-over-clause-transact-sql?view=sql-server-ver16\">Microsoft documentation</a>, determines the partitioning and ordering of a rowset before the associated window function is applied. That is, the OVER clause defines a window or user-specified set of rows within a query result\u00a0set.</p>\n<p>You can combine over() with aggregate functions like sum, avg/max/min, or with functions like rank and lag, which I\u2019ll explain\u00a0below.</p>\n<h4>row_number(), rank() and dense_rank():</h4>\n<p>These functions are useful when you need to \u201crank\u201d your results based on a partition (or not). Row_number, as the name says, will enumerate the rows based on the order defined, as will rank and dense_rank, with some differences:</p>\n<p>When using row_number, after ordering and enumerating the rows, it doesn\u2019t matter if the results are \u201ctied\u201d (like when you have two rows with the same values), each row will have a different and increasing number. When using rank(), duplicate values will have the same number but will count toward the total rank, and the rank number will \u201cleap\u201d when it reaches a non-duplicated row, leaving a gap in the rank\u00a0numbers.</p>\n<p>Lastly, with dense_rank(), duplicated rows will have the same number, but the rank will not \u201cleap\u201d when it reaches the next non-duplicated row, instead, it will continue with the number right next to the last rank number, leaving no gaps in the rank numbers, like the following example:</p>\n<pre>| STATE  | BUSHELS | RANK() | DENSE_RANK()|  ROW_NUMBER () |<br><br>| Kansas |  130    |   1    |      1      |      1         |<br>| Kansas |  120    |   2    |      2      |      2         | <br>| Iowa   |  110    |   3    |      3      |      3         | <br>| Iowa   |  110    |   3    |      3      |      4         |<br>| Iowa   |  100    |   5    |      4      |      5         |</pre>\n<h4>lag () and lead() functions:</h4>\n<p>Lag () window functions are useful when you need to access or perform a calculation based on a previous row, like when doing a Month over Month, WoW, or YoY comparison. Here\u2019s the\u00a0syntax:</p>\n<pre>LAG (scalar_expression [,offset] [,default]) <br>    OVER ( [ partition_by_clause ] order_by_clause )</pre>\n<p>The first argument is the value to be returned, offset is the number of rows back from the current row from which to obtain a value, and default is the value to return when <em>offset</em> is beyond the scope of the partition.</p>\n<p>Following is an example of calculating a difference in revenue from past years from the Snowflake <a href=\"https://docs.snowflake.com/en/sql-reference/functions/lag.html\">documentation</a>:</p>\n<pre>select emp_id, year, revenue, <br>       revenue - lag(revenue, 1, 0) over (partition by emp_id order by year) as diff_to_prev <br>    from sales <br>    order by emp_id, year;</pre>\n<pre>+--------+------+----------+--------------+<br>| EMP_ID | YEAR |  REVENUE | DIFF_TO_PREV |<br>|--------+------+----------+--------------|<br>|      0 | 2010 |  1000.00 |      1000.00 |<br>|      0 | 2011 |  1500.00 |       500.00 |<br>|      0 | 2012 |   500.00 |     -1000.00 |<br>|      0 | 2013 |   750.00 |       250.00 |<br>|      1 | 2010 | 10000.00 |     10000.00 |<br>|      1 | 2011 | 12500.00 |      2500.00 |<br>|      1 | 2012 | 15000.00 |      2500.00 |<br>|      1 | 2013 | 20000.00 |      5000.00 |<br>|      2 | 2012 |   500.00 |       500.00 |<br>|      2 | 2013 |   800.00 |       300.00 |<br>+--------+------+----------+--------------+</pre>\n<p>Lead is similar to lag(), but you can access a subsequent row instead of accessing previous\u00a0rows.</p>\n<h4>Qualify</h4>\n<p>When working with modern RDBMS like <a href=\"https://docs.snowflake.com/en/sql-reference/constructs/qualify.html\">Snowflake</a> and <a href=\"https://docs.databricks.com/sql/language-manual/sql-ref-syntax-qry-select-qualify.html\">Databricks</a>, you can use <em>Qualify</em> to filter out the results of a window function, just like using HAVING for filtering results of group by. The syntax is very\u00a0simple:</p>\n<pre>SELECT <em>&lt;column_list&gt;</em><br>  FROM <em>&lt;data_source&gt;</em><br>  [GROUP BY ...]<br>  [HAVING ...]<br>  QUALIFY boolean expression<br>  [ ... ]</pre>\n<p>That\u2019s all for\u00a0today!</p>\n<p>If you reached this far, thank you for reading, and follow me for more articles about SQL, Python, and Data Analytics!</p>\n<h4>Sources:</h4>\n<ul>\n<li><a href=\"https://docs.snowflake.com/en/\">Welcome to Snowflake Documentation - Snowflake Documentation</a></li>\n<li><a href=\"https://docs.microsoft.com/pt-br/sql/\">Documenta\u00e7\u00e3o do Microsoft SQL - SQL Server</a></li>\n<li><a href=\"https://docs.databricks.com/\">Databricks documentation</a></li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=32420009ec21\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*8VKYvZ_9ZnOLZbb9\"><figcaption>Photo by <a href=\"https://unsplash.com/@casparrubin?utm_source=medium&amp;utm_medium=referral\">Caspar Camille Rubin</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>Today, I\u2019m going to talk about some of the functions that I often use when I\u2019m working with SQL to query information for my analysis and dashboards.</p>\n<p>When using SQL for data analysis, some functions are very useful for dealing with everyday business problems.</p>\n<p>First, some basic functions don\u2019t require much explanation, and most SQL practitioners know how to use them. These functions are aggregate/mathematical functions (like sum, max/min, average, count, ceil), string manipulation functions (like lower, concat), or date manipulation functions (like extract or date_trunc).</p>\n<p>I\u2019m not going to go into detail about these functions since they are very straightforward, and you can learn their syntax by reading the documentation:</p>\n<p><a href=\"https://www.w3schools.com/sql/sql_ref_mysql.asp\">My SQL</a>, <a href=\"https://www.w3schools.com/sql/sql_ref_sqlserver.asp\">SQL Server</a>, <a href=\"https://docs.snowflake.com/en/\">Snowflake</a>, or <a href=\"https://www.postgresql.org/docs/current/functions.html\">PostgreSQL functions</a>. For other DBMS functions, you can look for their documentation on\u00a0Google.</p>\n<p>There are other types of very useful functions called <strong>Window functions</strong>. I\u2019m going to talk about these functions and their syntax, and also give examples of how to use\u00a0them.</p>\n<h4>SELECT\u200a\u2014\u200aOVER\u00a0Clause:</h4>\n<p>Select\u200a\u2014\u200aOVER, according to <a href=\"https://docs.microsoft.com/pt-br/sql/t-sql/queries/select-over-clause-transact-sql?view=sql-server-ver16\">Microsoft documentation</a>, determines the partitioning and ordering of a rowset before the associated window function is applied. That is, the OVER clause defines a window or user-specified set of rows within a query result\u00a0set.</p>\n<p>You can combine over() with aggregate functions like sum, avg/max/min, or with functions like rank and lag, which I\u2019ll explain\u00a0below.</p>\n<h4>row_number(), rank() and dense_rank():</h4>\n<p>These functions are useful when you need to \u201crank\u201d your results based on a partition (or not). Row_number, as the name says, will enumerate the rows based on the order defined, as will rank and dense_rank, with some differences:</p>\n<p>When using row_number, after ordering and enumerating the rows, it doesn\u2019t matter if the results are \u201ctied\u201d (like when you have two rows with the same values), each row will have a different and increasing number. When using rank(), duplicate values will have the same number but will count toward the total rank, and the rank number will \u201cleap\u201d when it reaches a non-duplicated row, leaving a gap in the rank\u00a0numbers.</p>\n<p>Lastly, with dense_rank(), duplicated rows will have the same number, but the rank will not \u201cleap\u201d when it reaches the next non-duplicated row, instead, it will continue with the number right next to the last rank number, leaving no gaps in the rank numbers, like the following example:</p>\n<pre>| STATE  | BUSHELS | RANK() | DENSE_RANK()|  ROW_NUMBER () |<br><br>| Kansas |  130    |   1    |      1      |      1         |<br>| Kansas |  120    |   2    |      2      |      2         | <br>| Iowa   |  110    |   3    |      3      |      3         | <br>| Iowa   |  110    |   3    |      3      |      4         |<br>| Iowa   |  100    |   5    |      4      |      5         |</pre>\n<h4>lag () and lead() functions:</h4>\n<p>Lag () window functions are useful when you need to access or perform a calculation based on a previous row, like when doing a Month over Month, WoW, or YoY comparison. Here\u2019s the\u00a0syntax:</p>\n<pre>LAG (scalar_expression [,offset] [,default]) <br>    OVER ( [ partition_by_clause ] order_by_clause )</pre>\n<p>The first argument is the value to be returned, offset is the number of rows back from the current row from which to obtain a value, and default is the value to return when <em>offset</em> is beyond the scope of the partition.</p>\n<p>Following is an example of calculating a difference in revenue from past years from the Snowflake <a href=\"https://docs.snowflake.com/en/sql-reference/functions/lag.html\">documentation</a>:</p>\n<pre>select emp_id, year, revenue, <br>       revenue - lag(revenue, 1, 0) over (partition by emp_id order by year) as diff_to_prev <br>    from sales <br>    order by emp_id, year;</pre>\n<pre>+--------+------+----------+--------------+<br>| EMP_ID | YEAR |  REVENUE | DIFF_TO_PREV |<br>|--------+------+----------+--------------|<br>|      0 | 2010 |  1000.00 |      1000.00 |<br>|      0 | 2011 |  1500.00 |       500.00 |<br>|      0 | 2012 |   500.00 |     -1000.00 |<br>|      0 | 2013 |   750.00 |       250.00 |<br>|      1 | 2010 | 10000.00 |     10000.00 |<br>|      1 | 2011 | 12500.00 |      2500.00 |<br>|      1 | 2012 | 15000.00 |      2500.00 |<br>|      1 | 2013 | 20000.00 |      5000.00 |<br>|      2 | 2012 |   500.00 |       500.00 |<br>|      2 | 2013 |   800.00 |       300.00 |<br>+--------+------+----------+--------------+</pre>\n<p>Lead is similar to lag(), but you can access a subsequent row instead of accessing previous\u00a0rows.</p>\n<h4>Qualify</h4>\n<p>When working with modern RDBMS like <a href=\"https://docs.snowflake.com/en/sql-reference/constructs/qualify.html\">Snowflake</a> and <a href=\"https://docs.databricks.com/sql/language-manual/sql-ref-syntax-qry-select-qualify.html\">Databricks</a>, you can use <em>Qualify</em> to filter out the results of a window function, just like using HAVING for filtering results of group by. The syntax is very\u00a0simple:</p>\n<pre>SELECT <em>&lt;column_list&gt;</em><br>  FROM <em>&lt;data_source&gt;</em><br>  [GROUP BY ...]<br>  [HAVING ...]<br>  QUALIFY boolean expression<br>  [ ... ]</pre>\n<p>That\u2019s all for\u00a0today!</p>\n<p>If you reached this far, thank you for reading, and follow me for more articles about SQL, Python, and Data Analytics!</p>\n<h4>Sources:</h4>\n<ul>\n<li><a href=\"https://docs.snowflake.com/en/\">Welcome to Snowflake Documentation - Snowflake Documentation</a></li>\n<li><a href=\"https://docs.microsoft.com/pt-br/sql/\">Documenta\u00e7\u00e3o do Microsoft SQL - SQL Server</a></li>\n<li><a href=\"https://docs.databricks.com/\">Databricks documentation</a></li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=32420009ec21\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["data-science","sql","data-visualization","data","analysis"]},{"title":"Data visualization with Python &amp; Plotly","pubDate":"2022-07-24 23:23:07","link":"https://medium.com/@alestamm/data-visualization-with-python-plotly-770bfd7e5854?source=rss-bdc9fb848ad5------2","guid":"https://medium.com/p/770bfd7e5854","author":"Alexandre Stamm","thumbnail":"https://cdn-images-1.medium.com/max/1024/0*JpZNn_BdJLDyw039","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*JpZNn_BdJLDyw039\"><figcaption>Photo by <a href=\"https://unsplash.com/@goumbik?utm_source=medium&amp;utm_medium=referral\">Lukas Blazek</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><h3>How to make beautiful charts &amp; dashboards using\u00a0Plotly</h3>\n<p>Hello everyone!</p>\n<p>Lately, I\u2019ve been using a lot more <a href=\"https://plotly.com/\">Plotly</a> than <a href=\"https://seaborn.pydata.org/\">Seaborn</a> for my Data Visualizations, for a few\u00a0reasons:</p>\n<p>\u00b7 Plotly charts are interactive, and you can make dashboards easily;</p>\n<p>\u00b7 Some things are much simpler to do in Plotly compared to Seaborn, like putting legend numbers in a line\u00a0plot;</p>\n<p>\u00b7 Usually when you provide an invalid parameter to a Plotly function/object, it returns a useful error message, with suggestions on what you might have\u00a0missed;</p>\n<p>\u00b7 Also, I think the default style of Plotly is better than the default style of\u00a0Seaborn.</p>\n<p>It\u2019s not like Seaborn doesn\u2019t have its advantages either, we all know how powerful and complete the Seaborn library is for data visualization. But lately, when I just need to plot some simple and beautiful charts, Plotly has been my go-to\u00a0library.</p>\n<p>That being said, today I\u2019m going to write some general tips &amp; tricks about Plotly, and some examples of what kind of visualizations we can make with this amazing\u00a0library!</p>\n<p>First, if you would like to learn Plotly more in-depth, from how the figure data works, to publishing a Dashboard app using <a href=\"https://dash.plotly.com/\">Dash</a>, I suggest reading the <a href=\"https://plotly.com/python/\">documentation</a>, which is very well written and complete.</p>\n<p>Now let\u2019s get\u00a0started!</p>\n<p>First, there are two main ways to plot graphs in Plotly, using <a href=\"https://plotly.com/python/plotly-express/\">Plotly Express</a> or <a href=\"https://plotly.com/python/graph-objects/\">Plotly Go</a> (Graph objects).</p>\n<p>I mostly use Plotly express when I need\u00a0to\u00a0plot\u00a0simple\u00a0charts, and use Go when I need more customization options.</p>\n<p>Plotting charts using Plotly Express is very simple, here are some examples:</p>\n<h4>\n<a href=\"https://plotly.com/python/bar-charts/\">Bar charts</a>:</h4>\n<p>To plot a bar chart with Plotly Express, first, we import a sample dataframe from the library, and then call the px.bar argument:</p>\n<pre>import plotly.express as px<br>data_canada = px.data.gapminder().query(\"country == 'Canada'\")<br>fig = px.bar(data_canada, x='year', y='pop')<br>fig.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/880/1*ue0hT2jri-oeD8BYGRjhFQ.png\"></figure><p>Now, say we want to add legend text inside our bars, it\u2019s as simple as adding the <strong>text_auto=True</strong> argument.</p>\n<pre>import plotly.express as px<br>df = px.data.medals_long()<br><br>fig = px.bar(df, x=\"medal\", y=\"count\", color=\"nation\", text_auto=<strong>True</strong>)<br>fig.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/887/1*yzwrS7mqQjq0reNXlVVjKw.png\"></figure><h4>\n<a href=\"https://plotly.com/python/line-charts/\">Line charts</a>:</h4>\n<p>Just like with our bar charts, we\u2019re going to import a sample df and use px.line to plot our\u00a0chart:</p>\n<pre>import plotly.express as px<br><br>df = px.data.gapminder().query(\"country=='Canada'\")<br>fig = px.line(df, x=\"year\", y=\"lifeExp\", title='Life expectancy in Canada')<br>fig.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/864/1*KYG4a9MlMajULmnJW0qYhA.png\"></figure><p>Now, to show text legends on our line chart, the code is a little different from the bar chart, but it\u2019s also very simple, we just have to specify where Plotly should get our numbers from (in this case, the \u201cyear\u201d column), and it\u2019s also good to use update_traces to specify where we want our text to\u00a0show:</p>\n<pre>import plotly.express as px<br><br>df = px.data.gapminder().query(\"country in ['Canada', 'Botswana']\")<br><br>fig = px.line(df, x=\"lifeExp\", y=\"gdpPercap\", color=\"country\", text=\"year\")<br>fig.update_traces(textposition=\"bottom right\")<br>fig.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/923/1*PdISTUlo0YaMGxpE5SO5Kw.png\"></figure><p>You can also make all other sorts of charts, like <a href=\"https://plotly.com/python/histograms/\">histograms</a>, <a href=\"https://plotly.com/python/distplot/\">distplots</a>, <a href=\"https://plotly.com/python/box-plots/\">boxplots</a>, <a href=\"https://plotly.com/python/heatmaps/\">heatmaps</a>, <a href=\"https://plotly.com/python/waterfall-charts/\">waterfall</a>\u2026 You can find a list of all kinds of charts you can plot\u00a0<a href=\"https://plotly.com/python/\">here</a>.</p>\n<h4>Using Plotly Graph Objects for more customizable graphics:</h4>\n<p>Now let\u2019s try something harder!</p>\n<p>Say we want to plot a Bar &amp; Line chart, but using a secondary Y axis for our line chart. Here\u2019s where the customization of Plotly Go comes in\u00a0hand!</p>\n<p>First, let\u2019s create a sample dataframe, with data from visitors in a public\u00a0park.</p>\n<a href=\"https://medium.com/media/8409f19b4a2683eba236cb5fe14fe6c7/href\">https://medium.com/media/8409f19b4a2683eba236cb5fe14fe6c7/href</a><p>Let\u2019s code our chart! I\u2019m going to explain how to do it step-by-step in the code\u00a0below:</p>\n<a href=\"https://medium.com/media/886a8491594eff9dc5b98fdd6cc77a66/href\">https://medium.com/media/886a8491594eff9dc5b98fdd6cc77a66/href</a><p>Here\u2019s how our chart looks\u00a0like:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*zHkRQe9HrMCqxeee4px0FA.png\"></figure><p>And we\u2019re\u00a0done!</p>\n<p>Since we did our chart with Plotly, we can also use Dash to turn it into a dashboard/interactive chart!</p>\n<p>There\u2019s a very useful guide on how to deploy your dashboard for free using Heroku that you can find\u00a0<a href=\"https://dash.plotly.com/deployment\">here</a>.</p>\n<p>That\u2019s it for today! If you liked this guide, please leave your comment and follow me for more Data Visualization tips/tutorials!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=770bfd7e5854\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*JpZNn_BdJLDyw039\"><figcaption>Photo by <a href=\"https://unsplash.com/@goumbik?utm_source=medium&amp;utm_medium=referral\">Lukas Blazek</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><h3>How to make beautiful charts &amp; dashboards using\u00a0Plotly</h3>\n<p>Hello everyone!</p>\n<p>Lately, I\u2019ve been using a lot more <a href=\"https://plotly.com/\">Plotly</a> than <a href=\"https://seaborn.pydata.org/\">Seaborn</a> for my Data Visualizations, for a few\u00a0reasons:</p>\n<p>\u00b7 Plotly charts are interactive, and you can make dashboards easily;</p>\n<p>\u00b7 Some things are much simpler to do in Plotly compared to Seaborn, like putting legend numbers in a line\u00a0plot;</p>\n<p>\u00b7 Usually when you provide an invalid parameter to a Plotly function/object, it returns a useful error message, with suggestions on what you might have\u00a0missed;</p>\n<p>\u00b7 Also, I think the default style of Plotly is better than the default style of\u00a0Seaborn.</p>\n<p>It\u2019s not like Seaborn doesn\u2019t have its advantages either, we all know how powerful and complete the Seaborn library is for data visualization. But lately, when I just need to plot some simple and beautiful charts, Plotly has been my go-to\u00a0library.</p>\n<p>That being said, today I\u2019m going to write some general tips &amp; tricks about Plotly, and some examples of what kind of visualizations we can make with this amazing\u00a0library!</p>\n<p>First, if you would like to learn Plotly more in-depth, from how the figure data works, to publishing a Dashboard app using <a href=\"https://dash.plotly.com/\">Dash</a>, I suggest reading the <a href=\"https://plotly.com/python/\">documentation</a>, which is very well written and complete.</p>\n<p>Now let\u2019s get\u00a0started!</p>\n<p>First, there are two main ways to plot graphs in Plotly, using <a href=\"https://plotly.com/python/plotly-express/\">Plotly Express</a> or <a href=\"https://plotly.com/python/graph-objects/\">Plotly Go</a> (Graph objects).</p>\n<p>I mostly use Plotly express when I need\u00a0to\u00a0plot\u00a0simple\u00a0charts, and use Go when I need more customization options.</p>\n<p>Plotting charts using Plotly Express is very simple, here are some examples:</p>\n<h4>\n<a href=\"https://plotly.com/python/bar-charts/\">Bar charts</a>:</h4>\n<p>To plot a bar chart with Plotly Express, first, we import a sample dataframe from the library, and then call the px.bar argument:</p>\n<pre>import plotly.express as px<br>data_canada = px.data.gapminder().query(\"country == 'Canada'\")<br>fig = px.bar(data_canada, x='year', y='pop')<br>fig.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/880/1*ue0hT2jri-oeD8BYGRjhFQ.png\"></figure><p>Now, say we want to add legend text inside our bars, it\u2019s as simple as adding the <strong>text_auto=True</strong> argument.</p>\n<pre>import plotly.express as px<br>df = px.data.medals_long()<br><br>fig = px.bar(df, x=\"medal\", y=\"count\", color=\"nation\", text_auto=<strong>True</strong>)<br>fig.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/887/1*yzwrS7mqQjq0reNXlVVjKw.png\"></figure><h4>\n<a href=\"https://plotly.com/python/line-charts/\">Line charts</a>:</h4>\n<p>Just like with our bar charts, we\u2019re going to import a sample df and use px.line to plot our\u00a0chart:</p>\n<pre>import plotly.express as px<br><br>df = px.data.gapminder().query(\"country=='Canada'\")<br>fig = px.line(df, x=\"year\", y=\"lifeExp\", title='Life expectancy in Canada')<br>fig.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/864/1*KYG4a9MlMajULmnJW0qYhA.png\"></figure><p>Now, to show text legends on our line chart, the code is a little different from the bar chart, but it\u2019s also very simple, we just have to specify where Plotly should get our numbers from (in this case, the \u201cyear\u201d column), and it\u2019s also good to use update_traces to specify where we want our text to\u00a0show:</p>\n<pre>import plotly.express as px<br><br>df = px.data.gapminder().query(\"country in ['Canada', 'Botswana']\")<br><br>fig = px.line(df, x=\"lifeExp\", y=\"gdpPercap\", color=\"country\", text=\"year\")<br>fig.update_traces(textposition=\"bottom right\")<br>fig.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/923/1*PdISTUlo0YaMGxpE5SO5Kw.png\"></figure><p>You can also make all other sorts of charts, like <a href=\"https://plotly.com/python/histograms/\">histograms</a>, <a href=\"https://plotly.com/python/distplot/\">distplots</a>, <a href=\"https://plotly.com/python/box-plots/\">boxplots</a>, <a href=\"https://plotly.com/python/heatmaps/\">heatmaps</a>, <a href=\"https://plotly.com/python/waterfall-charts/\">waterfall</a>\u2026 You can find a list of all kinds of charts you can plot\u00a0<a href=\"https://plotly.com/python/\">here</a>.</p>\n<h4>Using Plotly Graph Objects for more customizable graphics:</h4>\n<p>Now let\u2019s try something harder!</p>\n<p>Say we want to plot a Bar &amp; Line chart, but using a secondary Y axis for our line chart. Here\u2019s where the customization of Plotly Go comes in\u00a0hand!</p>\n<p>First, let\u2019s create a sample dataframe, with data from visitors in a public\u00a0park.</p>\n<a href=\"https://medium.com/media/8409f19b4a2683eba236cb5fe14fe6c7/href\">https://medium.com/media/8409f19b4a2683eba236cb5fe14fe6c7/href</a><p>Let\u2019s code our chart! I\u2019m going to explain how to do it step-by-step in the code\u00a0below:</p>\n<a href=\"https://medium.com/media/886a8491594eff9dc5b98fdd6cc77a66/href\">https://medium.com/media/886a8491594eff9dc5b98fdd6cc77a66/href</a><p>Here\u2019s how our chart looks\u00a0like:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*zHkRQe9HrMCqxeee4px0FA.png\"></figure><p>And we\u2019re\u00a0done!</p>\n<p>Since we did our chart with Plotly, we can also use Dash to turn it into a dashboard/interactive chart!</p>\n<p>There\u2019s a very useful guide on how to deploy your dashboard for free using Heroku that you can find\u00a0<a href=\"https://dash.plotly.com/deployment\">here</a>.</p>\n<p>That\u2019s it for today! If you liked this guide, please leave your comment and follow me for more Data Visualization tips/tutorials!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=770bfd7e5854\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["data-science","plotly","pandas","data-visualization","python"]},{"title":"Checking and cleaning data in Python with Pandas","pubDate":"2022-06-19 23:33:27","link":"https://medium.com/@alestamm/checking-and-cleaning-data-in-python-with-pandas-c955f45f0f7?source=rss-bdc9fb848ad5------2","guid":"https://medium.com/p/c955f45f0f7","author":"Alexandre Stamm","thumbnail":"https://cdn-images-1.medium.com/max/1024/0*N3e_Ft7TrzAVcZp0","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*N3e_Ft7TrzAVcZp0\"><figcaption>Photo by <a href=\"https://unsplash.com/@towfiqu999999?utm_source=medium&amp;utm_medium=referral\">Towfiqu barbhuiya</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>Hello everyone! In my previous articles, we learned about how to <a href=\"https://medium.com/@alestamm/how-to-load-data-into-a-pandas-dataframe-2f1a3586e415\">load into a Pandas DataFrame</a>, and how to <a href=\"https://medium.com/@alestamm/importing-data-from-a-postgresql-database-to-a-pandas-dataframe-5f4bffcd8bb2\">Import data from a PostgreSQL database to a Pandas DataFrame</a>.</p>\n<p>You may have heard many times that Data scientists spend most of their time cleaning and organizing data. That may change depending on what type of data you work on, but regardless of how messy the data of your company/project is, organizing our data is one of the most important parts of a Data Scientist/Data Analyst\u00a0job.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*hCYX87DYpXJ5pzby\"><figcaption>Photo by <a href=\"https://unsplash.com/@dav420?utm_source=medium&amp;utm_medium=referral\">David Pupaza</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>According to a <a href=\"https://hbr.org/2016/09/bad-data-costs-the-u-s-3-trillion-per-year\">report released by IBM in 2016</a>, the cost of bad data to U.S. businesses and organizations was around 3.1 trillion dollars per year, and a big source of bad data is, guess what? Human\u00a0error!</p>\n<p>So today we\u2019re going to dive a little deeper into our data and use Pandas and some Python functions to clean and organize our\u00a0data.</p>\n<p>First, when working on a dataset, before you start cleaning and organizing your data, it\u2019s good practice to get a first look at what we\u2019re dealing\u00a0with.</p>\n<p>To do this, I generally use the following functions, and I\u2019m going to pass over them quickly since they are really\u00a0basic:</p>\n<pre>import pandas as pd</pre>\n<pre># Print the first 5 rows (or whatever number you pass inside the parentheses) of the DataFrame<br>df.head()<br>df.tail() # same as .head() but returns the last rows</pre>\n<pre># Summarize the central tendency, dispersion, and shape of a dataset\u2019s distribution, excluding NaN values<br>df.describe()</pre>\n<pre># Checking the dimensionality of the DataFrame<br>df.shape</pre>\n<pre># Checking data types of the DataFrame<br>df.dtypes</pre>\n<pre># Printing a concise summary of a DataFrame<br>df.info()</pre>\n<p>From here, I usually plot some graphics to get a feeling of what we\u2019re dealing with. Graphics are a good way to check the distribution and possible outliers of a dataset. A good library to plot data in python is <a href=\"https://seaborn.pydata.org/index.html\">Seaborn</a>. Seaborn is a Python data visualization library based on <a href=\"https://matplotlib.org/\">matplotlib</a>. It provides a high-level interface for drawing attractive and informative statistical graphics.</p>\n<p>The canonical import of\u00a0seaborn:</p>\n<pre>import seaborn as sns</pre>\n<p>We can use <a href=\"https://seaborn.pydata.org/generated/seaborn.histplot.html\">sns.histplot()</a> to plot a histogram and check distribution, <a href=\"https://seaborn.pydata.org/generated/seaborn.scatterplot.html#seaborn.scatterplot\">sns.scatterplot() </a>and <a href=\"https://seaborn.pydata.org/generated/seaborn.lineplot.html#seaborn.lineplot\">sns.lineplot</a> to check for statistical relationships, or <a href=\"https://seaborn.pydata.org/generated/seaborn.boxplot.html#seaborn.boxplot\">sns.boxplot()</a>, <a href=\"https://seaborn.pydata.org/generated/seaborn.barplot.html#seaborn.barplot\">sns.barplot()</a>, and <a href=\"https://seaborn.pydata.org/generated/seaborn.countplot.html#seaborn.countplot\">sns.countplot()</a> for categorical variables.</p>\n<p>Ok! After we\u2019re done with all that, we can start cleaning our data and preparing our analysis.</p>\n<p>First, everyone that works as a Data/Business Analyst or Data Scientist knows that a lot of problems when dealing with data are caused by null\u00a0values.</p>\n<p>Pandas has a lot of useful functions to deal with Na\u2019s values. The most common ones\u00a0are:</p>\n<p><a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html\">.dropna()</a>: Remove missing\u00a0values.</p>\n<p><a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html\">.filllna()</a>: Fill NA/NaN values using the specified method.</p>\n<p>Using\u00a0.fillna(), you can pass the value directly to the function:</p>\n<pre>df.fillna(\u2018None\u2019)<br>or<br>df.fillna(0)<br>or<br>df.fillna('2022-06-01')</pre>\n<p>Or you can use forward(ffill) and back fill(bfill) methods:</p>\n<pre># Ffill propagates the last valid observation forward to the next valid one.<br>df.fillna(method=ffill)</pre>\n<pre># Bfill uses the next valid observation to fill the gap.<br>df.fillna(method=\u201dbfill\u201d):</pre>\n<p>After dealing with nulls, I usually check the data types of our dataset so we have every column with its correct type. Sometimes dates and numbers can come as strings, and it\u2019s better to deal with this right at the beginning.</p>\n<p>For dealing with data types, I mostly use the following functions:</p>\n<p><a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html\">.astype()</a>:</p>\n<p>Cast a pandas object to a specified dtype.</p>\n<p><a href=\"https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html\">pd.to_datetime()</a>:</p>\n<p>Convert argument to datetime.</p>\n<p>Now, after dealing with nulls and wrong data types, we usually begin to dive deeper into the data itself. Here we have a lot of options on how to clean/organize our\u00a0data.</p>\n<p>Sometimes it\u2019s useful when dealing with categorical data to create clusters so we can aggregate our data in a way that makes\u00a0sense.</p>\n<p>One of these days I was dealing with some data from Google Analytics and had to narrow down our access sources for the analysis. When doing something like this, you can create a custom function and apply it to a column of your DataFrame. Here\u2019s an\u00a0example:</p>\n<pre>def clustering(x):</pre>\n<pre>   if 'google / cpc' in x or 'Google / CPC' in x or 'adwords' in x:</pre>\n<pre>      return 'Google'</pre>\n<pre>   elif 'fb' in x or 'facebook' in x:</pre>\n<pre>      return 'Facebook'</pre>\n<pre>``` Here we're going to use a list created beforehand and #check our dataframe for instances of our list. Since filtering strings like this is case-sensitive, we use .lower() to convert all strings to lowercase ```</pre>\n<pre>   elif any([k.lower() in x.lower() for k in list]):<br>      return 'In list'</pre>\n<pre>   elif 'insta' in x or 'ig' in x:</pre>\n<pre>      return 'Instagram'<br>   else: <br>      return 'Others'</pre>\n<p>Now that we have our function, we can use\u00a0<a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html\">.apply</a> to apply it to our DataFrame/column:</p>\n<pre>df['column'] = df['column'].apply(lambda x: clustering(x))</pre>\n<p>And just like that, our function will go over each value on our column and apply the clustering we\u00a0defined.</p>\n<p>From here, I usually start organizing, grouping, and feature engineering our data, and I\u2019m going to write about those steps in the near\u00a0future.</p>\n<p>If you read this far, thank you for your time and I hope I\u2019ve managed to give you some insights on how to start dealing with data before doing your analysis!</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*PEbapZA-1ZLZ9dkd\"><figcaption>Photo by <a href=\"https://unsplash.com/@craft_ear?utm_source=medium&amp;utm_medium=referral\">Jan Tinneberg</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c955f45f0f7\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*N3e_Ft7TrzAVcZp0\"><figcaption>Photo by <a href=\"https://unsplash.com/@towfiqu999999?utm_source=medium&amp;utm_medium=referral\">Towfiqu barbhuiya</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>Hello everyone! In my previous articles, we learned about how to <a href=\"https://medium.com/@alestamm/how-to-load-data-into-a-pandas-dataframe-2f1a3586e415\">load into a Pandas DataFrame</a>, and how to <a href=\"https://medium.com/@alestamm/importing-data-from-a-postgresql-database-to-a-pandas-dataframe-5f4bffcd8bb2\">Import data from a PostgreSQL database to a Pandas DataFrame</a>.</p>\n<p>You may have heard many times that Data scientists spend most of their time cleaning and organizing data. That may change depending on what type of data you work on, but regardless of how messy the data of your company/project is, organizing our data is one of the most important parts of a Data Scientist/Data Analyst\u00a0job.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*hCYX87DYpXJ5pzby\"><figcaption>Photo by <a href=\"https://unsplash.com/@dav420?utm_source=medium&amp;utm_medium=referral\">David Pupaza</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>According to a <a href=\"https://hbr.org/2016/09/bad-data-costs-the-u-s-3-trillion-per-year\">report released by IBM in 2016</a>, the cost of bad data to U.S. businesses and organizations was around 3.1 trillion dollars per year, and a big source of bad data is, guess what? Human\u00a0error!</p>\n<p>So today we\u2019re going to dive a little deeper into our data and use Pandas and some Python functions to clean and organize our\u00a0data.</p>\n<p>First, when working on a dataset, before you start cleaning and organizing your data, it\u2019s good practice to get a first look at what we\u2019re dealing\u00a0with.</p>\n<p>To do this, I generally use the following functions, and I\u2019m going to pass over them quickly since they are really\u00a0basic:</p>\n<pre>import pandas as pd</pre>\n<pre># Print the first 5 rows (or whatever number you pass inside the parentheses) of the DataFrame<br>df.head()<br>df.tail() # same as .head() but returns the last rows</pre>\n<pre># Summarize the central tendency, dispersion, and shape of a dataset\u2019s distribution, excluding NaN values<br>df.describe()</pre>\n<pre># Checking the dimensionality of the DataFrame<br>df.shape</pre>\n<pre># Checking data types of the DataFrame<br>df.dtypes</pre>\n<pre># Printing a concise summary of a DataFrame<br>df.info()</pre>\n<p>From here, I usually plot some graphics to get a feeling of what we\u2019re dealing with. Graphics are a good way to check the distribution and possible outliers of a dataset. A good library to plot data in python is <a href=\"https://seaborn.pydata.org/index.html\">Seaborn</a>. Seaborn is a Python data visualization library based on <a href=\"https://matplotlib.org/\">matplotlib</a>. It provides a high-level interface for drawing attractive and informative statistical graphics.</p>\n<p>The canonical import of\u00a0seaborn:</p>\n<pre>import seaborn as sns</pre>\n<p>We can use <a href=\"https://seaborn.pydata.org/generated/seaborn.histplot.html\">sns.histplot()</a> to plot a histogram and check distribution, <a href=\"https://seaborn.pydata.org/generated/seaborn.scatterplot.html#seaborn.scatterplot\">sns.scatterplot() </a>and <a href=\"https://seaborn.pydata.org/generated/seaborn.lineplot.html#seaborn.lineplot\">sns.lineplot</a> to check for statistical relationships, or <a href=\"https://seaborn.pydata.org/generated/seaborn.boxplot.html#seaborn.boxplot\">sns.boxplot()</a>, <a href=\"https://seaborn.pydata.org/generated/seaborn.barplot.html#seaborn.barplot\">sns.barplot()</a>, and <a href=\"https://seaborn.pydata.org/generated/seaborn.countplot.html#seaborn.countplot\">sns.countplot()</a> for categorical variables.</p>\n<p>Ok! After we\u2019re done with all that, we can start cleaning our data and preparing our analysis.</p>\n<p>First, everyone that works as a Data/Business Analyst or Data Scientist knows that a lot of problems when dealing with data are caused by null\u00a0values.</p>\n<p>Pandas has a lot of useful functions to deal with Na\u2019s values. The most common ones\u00a0are:</p>\n<p><a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html\">.dropna()</a>: Remove missing\u00a0values.</p>\n<p><a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html\">.filllna()</a>: Fill NA/NaN values using the specified method.</p>\n<p>Using\u00a0.fillna(), you can pass the value directly to the function:</p>\n<pre>df.fillna(\u2018None\u2019)<br>or<br>df.fillna(0)<br>or<br>df.fillna('2022-06-01')</pre>\n<p>Or you can use forward(ffill) and back fill(bfill) methods:</p>\n<pre># Ffill propagates the last valid observation forward to the next valid one.<br>df.fillna(method=ffill)</pre>\n<pre># Bfill uses the next valid observation to fill the gap.<br>df.fillna(method=\u201dbfill\u201d):</pre>\n<p>After dealing with nulls, I usually check the data types of our dataset so we have every column with its correct type. Sometimes dates and numbers can come as strings, and it\u2019s better to deal with this right at the beginning.</p>\n<p>For dealing with data types, I mostly use the following functions:</p>\n<p><a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html\">.astype()</a>:</p>\n<p>Cast a pandas object to a specified dtype.</p>\n<p><a href=\"https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html\">pd.to_datetime()</a>:</p>\n<p>Convert argument to datetime.</p>\n<p>Now, after dealing with nulls and wrong data types, we usually begin to dive deeper into the data itself. Here we have a lot of options on how to clean/organize our\u00a0data.</p>\n<p>Sometimes it\u2019s useful when dealing with categorical data to create clusters so we can aggregate our data in a way that makes\u00a0sense.</p>\n<p>One of these days I was dealing with some data from Google Analytics and had to narrow down our access sources for the analysis. When doing something like this, you can create a custom function and apply it to a column of your DataFrame. Here\u2019s an\u00a0example:</p>\n<pre>def clustering(x):</pre>\n<pre>   if 'google / cpc' in x or 'Google / CPC' in x or 'adwords' in x:</pre>\n<pre>      return 'Google'</pre>\n<pre>   elif 'fb' in x or 'facebook' in x:</pre>\n<pre>      return 'Facebook'</pre>\n<pre>``` Here we're going to use a list created beforehand and #check our dataframe for instances of our list. Since filtering strings like this is case-sensitive, we use .lower() to convert all strings to lowercase ```</pre>\n<pre>   elif any([k.lower() in x.lower() for k in list]):<br>      return 'In list'</pre>\n<pre>   elif 'insta' in x or 'ig' in x:</pre>\n<pre>      return 'Instagram'<br>   else: <br>      return 'Others'</pre>\n<p>Now that we have our function, we can use\u00a0<a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html\">.apply</a> to apply it to our DataFrame/column:</p>\n<pre>df['column'] = df['column'].apply(lambda x: clustering(x))</pre>\n<p>And just like that, our function will go over each value on our column and apply the clustering we\u00a0defined.</p>\n<p>From here, I usually start organizing, grouping, and feature engineering our data, and I\u2019m going to write about those steps in the near\u00a0future.</p>\n<p>If you read this far, thank you for your time and I hope I\u2019ve managed to give you some insights on how to start dealing with data before doing your analysis!</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*PEbapZA-1ZLZ9dkd\"><figcaption>Photo by <a href=\"https://unsplash.com/@craft_ear?utm_source=medium&amp;utm_medium=referral\">Jan Tinneberg</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c955f45f0f7\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["python","pandas","data-science","data","data-visualization"]},{"title":"How to load data into a Pandas DataFrame","pubDate":"2022-06-16 20:33:16","link":"https://medium.com/@alestamm/how-to-load-data-into-a-pandas-dataframe-2f1a3586e415?source=rss-bdc9fb848ad5------2","guid":"https://medium.com/p/2f1a3586e415","author":"Alexandre Stamm","thumbnail":"https://cdn-images-1.medium.com/max/1024/0*X2Bfp-_R-oLBySt1","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*X2Bfp-_R-oLBySt1\"><figcaption>Photo by <a href=\"https://unsplash.com/@fantasyflip?utm_source=medium&amp;utm_medium=referral\">Philipp Katzenberger</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>In my next articles, I\u2019m going to write some tips &amp; tricks about Pandas, from basic to advanced!</p>\n<p>I\u2019m going to keep it short and refer to the documentation if you need a more detailed explanation of how things\u00a0work!</p>\n<p>Today we\u2019re going to start with the basics: How to load data into a Pandas DataFrame!</p>\n<h4><strong>Loading data into a Pandas DataFrame:</strong></h4>\n<p>In Pandas, you can create DataFrames using many different ways, like importing data from CSV, JSON, Excel, HTML, XML, and many other file types, or loading data from a SQL or Google BigQuery database.</p>\n<p>You can check all the functions for loading DataFrames on this page of the Pandas <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/io.html\">documentation</a>.</p>\n<p>In this tip, I\u2019m going to focus on the 2 main ways I load data into Pandas DataFrames: reading from CSV or connecting directly to an SQL database.</p>\n<h4>Importing data from an csv\u00a0file:</h4>\n<p>To import data from a csv file use <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html#:~:text=If%20True%20and%20parse_dates%20is,parsing%20speed%20by%205-10x.\">pd.read_csv()</a>:</p>\n<p>If your file is in the same folder as your notebook, the easiest way to use pd.read_csv() is just to pass the filename directly to the function:</p>\n<pre>df = pd.read_csv (\u2018file_name.csv\u2019)</pre>\n<p>In case your file is not in the same folder as your notebook, you pass the file path (absolute or relative) to the function:</p>\n<pre>#Absolute path:<br>df = pd.read_csv(\u2018C:/path/to/you/file/file_name.csv\u2019)</pre>\n<pre>#Relative path:<br>df = pd.read_csv(\u2018\u2026/ file_name.csv\u2019)</pre>\n<p>When passing just the file name to the function, Pandas will infer most of the information it needs, like separators, columns data types, and indexes. But you can also use the function parameters to better deal with the data you\u2019re going to\u00a0import.</p>\n<p>There are a series of parameters you can pass to <strong>read_csv</strong>, but the ones I use the most\u00a0are:</p>\n<blockquote>\n<strong>header:</strong> Row number(s) to use as the column names, and the start of the data. The default behavior is to infer the column\u00a0names.</blockquote>\n<blockquote>\n<strong>sep: </strong>Delimiter to use, default is a\u00a0comma.</blockquote>\n<blockquote>\n<strong>index_col: </strong>Column(s) to use as the row labels of the DataFrame.</blockquote>\n<blockquote>\n<strong>dtype: </strong>Data type for data or columns, when you want to explicitly inform the dtype of columns when importing,<strong> E.g.</strong> {\u2018a\u2019: np.float64}.</blockquote>\n<blockquote>\n<strong>na_values: </strong>Additional strings to recognize as NA/NaN. Say you are importing a CSV where there are \u201cNone\u201d values, you can pass na_values=[\u2018None\u2019] and pandas will convert them to NaN when importing.</blockquote>\n<blockquote>\n<strong>parse_dates and infer_datetime_format: </strong>Useful when dealing with date columns. To keep this short I suggest looking at the <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html#:~:text=If%20True%20and%20parse_dates%20is,parsing%20speed%20by%205-10x.\">documentation</a> if you have to use these parameters.</blockquote>\n<h4><strong>Importing data from an SQL Database:</strong></h4>\n<p>The two main ways I import data from SQL databases are:</p>\n<p><strong>Importing from PostgreSQL databases:</strong></p>\n<p>Follow my guide for <a href=\"https://medium.com/@alestamm/importing-data-from-a-postgresql-database-to-a-pandas-dataframe-5f4bffcd8bb2\">importing data from a PostgreSQL database to a Pandas DataFrame</a>.</p>\n<p><strong>Importing data from SQLite databases:</strong></p>\n<p>To import data from an SQLite Database, I use a really helpful library called <strong>sqlite3</strong>(<a href=\"https://docs.python.org/3/library/sqlite3.html#module-sqlite3\">docs</a>).</p>\n<p>You can read the documentation and also use the guide I made for <a href=\"https://medium.com/@alestamm/importing-data-from-a-postgresql-database-to-a-pandas-dataframe-5f4bffcd8bb2\">importing data from PostgreSQL database </a>as a reference since the functions are very\u00a0similar.</p>\n<h4>Aside from importing data, you can also create DataFrames using these functions:</h4>\n<p>Using pd.DataFrame.from_dict() and passing a dictionary:</p>\n<pre>data = {\u2018col_1\u2019: [3, 2, 1, 0], \u2018col_2\u2019: [\u2018a\u2019, \u2018b\u2019, \u2018c\u2019, \u2018d\u2019]}<br>pd.DataFrame.from_dict(data)</pre>\n<p>If you are not running your notebook on the cloud (like on <a href=\"https://colab.research.google.com/notebooks/intro.ipynb?hl=pt_BR\">Google Colab</a>), you can read the data directly from your clipboard using <strong>pd.read_clipboard().</strong></p>\n<p>You can also create an DataFrame from a list or numpy array using <strong>pd.DataFrame():</strong></p>\n<pre>pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])</pre>\n<pre>or</pre>\n<pre>pd.DataFrame(list)</pre>\n<p>That\u2019s all for\u00a0today!</p>\n<p>Follow me to read my upcoming tips for working with\u00a0Pandas!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=2f1a3586e415\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*X2Bfp-_R-oLBySt1\"><figcaption>Photo by <a href=\"https://unsplash.com/@fantasyflip?utm_source=medium&amp;utm_medium=referral\">Philipp Katzenberger</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>In my next articles, I\u2019m going to write some tips &amp; tricks about Pandas, from basic to advanced!</p>\n<p>I\u2019m going to keep it short and refer to the documentation if you need a more detailed explanation of how things\u00a0work!</p>\n<p>Today we\u2019re going to start with the basics: How to load data into a Pandas DataFrame!</p>\n<h4><strong>Loading data into a Pandas DataFrame:</strong></h4>\n<p>In Pandas, you can create DataFrames using many different ways, like importing data from CSV, JSON, Excel, HTML, XML, and many other file types, or loading data from a SQL or Google BigQuery database.</p>\n<p>You can check all the functions for loading DataFrames on this page of the Pandas <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/io.html\">documentation</a>.</p>\n<p>In this tip, I\u2019m going to focus on the 2 main ways I load data into Pandas DataFrames: reading from CSV or connecting directly to an SQL database.</p>\n<h4>Importing data from an csv\u00a0file:</h4>\n<p>To import data from a csv file use <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html#:~:text=If%20True%20and%20parse_dates%20is,parsing%20speed%20by%205-10x.\">pd.read_csv()</a>:</p>\n<p>If your file is in the same folder as your notebook, the easiest way to use pd.read_csv() is just to pass the filename directly to the function:</p>\n<pre>df = pd.read_csv (\u2018file_name.csv\u2019)</pre>\n<p>In case your file is not in the same folder as your notebook, you pass the file path (absolute or relative) to the function:</p>\n<pre>#Absolute path:<br>df = pd.read_csv(\u2018C:/path/to/you/file/file_name.csv\u2019)</pre>\n<pre>#Relative path:<br>df = pd.read_csv(\u2018\u2026/ file_name.csv\u2019)</pre>\n<p>When passing just the file name to the function, Pandas will infer most of the information it needs, like separators, columns data types, and indexes. But you can also use the function parameters to better deal with the data you\u2019re going to\u00a0import.</p>\n<p>There are a series of parameters you can pass to <strong>read_csv</strong>, but the ones I use the most\u00a0are:</p>\n<blockquote>\n<strong>header:</strong> Row number(s) to use as the column names, and the start of the data. The default behavior is to infer the column\u00a0names.</blockquote>\n<blockquote>\n<strong>sep: </strong>Delimiter to use, default is a\u00a0comma.</blockquote>\n<blockquote>\n<strong>index_col: </strong>Column(s) to use as the row labels of the DataFrame.</blockquote>\n<blockquote>\n<strong>dtype: </strong>Data type for data or columns, when you want to explicitly inform the dtype of columns when importing,<strong> E.g.</strong> {\u2018a\u2019: np.float64}.</blockquote>\n<blockquote>\n<strong>na_values: </strong>Additional strings to recognize as NA/NaN. Say you are importing a CSV where there are \u201cNone\u201d values, you can pass na_values=[\u2018None\u2019] and pandas will convert them to NaN when importing.</blockquote>\n<blockquote>\n<strong>parse_dates and infer_datetime_format: </strong>Useful when dealing with date columns. To keep this short I suggest looking at the <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html#:~:text=If%20True%20and%20parse_dates%20is,parsing%20speed%20by%205-10x.\">documentation</a> if you have to use these parameters.</blockquote>\n<h4><strong>Importing data from an SQL Database:</strong></h4>\n<p>The two main ways I import data from SQL databases are:</p>\n<p><strong>Importing from PostgreSQL databases:</strong></p>\n<p>Follow my guide for <a href=\"https://medium.com/@alestamm/importing-data-from-a-postgresql-database-to-a-pandas-dataframe-5f4bffcd8bb2\">importing data from a PostgreSQL database to a Pandas DataFrame</a>.</p>\n<p><strong>Importing data from SQLite databases:</strong></p>\n<p>To import data from an SQLite Database, I use a really helpful library called <strong>sqlite3</strong>(<a href=\"https://docs.python.org/3/library/sqlite3.html#module-sqlite3\">docs</a>).</p>\n<p>You can read the documentation and also use the guide I made for <a href=\"https://medium.com/@alestamm/importing-data-from-a-postgresql-database-to-a-pandas-dataframe-5f4bffcd8bb2\">importing data from PostgreSQL database </a>as a reference since the functions are very\u00a0similar.</p>\n<h4>Aside from importing data, you can also create DataFrames using these functions:</h4>\n<p>Using pd.DataFrame.from_dict() and passing a dictionary:</p>\n<pre>data = {\u2018col_1\u2019: [3, 2, 1, 0], \u2018col_2\u2019: [\u2018a\u2019, \u2018b\u2019, \u2018c\u2019, \u2018d\u2019]}<br>pd.DataFrame.from_dict(data)</pre>\n<p>If you are not running your notebook on the cloud (like on <a href=\"https://colab.research.google.com/notebooks/intro.ipynb?hl=pt_BR\">Google Colab</a>), you can read the data directly from your clipboard using <strong>pd.read_clipboard().</strong></p>\n<p>You can also create an DataFrame from a list or numpy array using <strong>pd.DataFrame():</strong></p>\n<pre>pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])</pre>\n<pre>or</pre>\n<pre>pd.DataFrame(list)</pre>\n<p>That\u2019s all for\u00a0today!</p>\n<p>Follow me to read my upcoming tips for working with\u00a0Pandas!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=2f1a3586e415\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["data-visualization","python","database","pandas","data-science"]},{"title":"Importing data from a PostgreSQL database to a Pandas DataFrame","pubDate":"2022-06-11 20:47:21","link":"https://medium.com/@alestamm/importing-data-from-a-postgresql-database-to-a-pandas-dataframe-5f4bffcd8bb2?source=rss-bdc9fb848ad5------2","guid":"https://medium.com/p/5f4bffcd8bb2","author":"Alexandre Stamm","thumbnail":"https://cdn-images-1.medium.com/max/1024/1*vE2qMiigkkMGdNf1vqNHbw.jpeg","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*vE2qMiigkkMGdNf1vqNHbw.jpeg\"><figcaption>Photo by <a href=\"https://unsplash.com/@purzlbaum\">Claudio Schwarz</a> on\u00a0<a href=\"https://unsplash.com/\">unsplash</a></figcaption></figure><h3>In this article, we\u2019ll go over how to create a pandas DataFrame using a simple connection and query to fetch data from a PostgreSQL database that requires authentication.</h3>\n<p>Most people that work with data know that SQL is great for working with tabular\u00a0data.</p>\n<p>Depending on what company you are working for, you may not have access to no-code BI tools like Tableau or PowerBI. That means that when doing your analysis, you will probably have to perform all transformations &amp; aggregations inside your DBMS and export data to a\u00a0.csv file and do the visualizations in Excel or Google Sheets. But if you know Python, you\u2019ll probably want to work with the dataset on Jupyter notebook with Pandas,\u00a0right?</p>\n<p>To work with PostgreSQL databases in Python I use <a href=\"https://pypi.org/project/psycopg2/\"><strong>psycopg2</strong></a>, which is the most popular PostgreSQL database adapter for\u00a0Python.</p>\n<p>The documentation for psycopg2 is found here: <a href=\"https://www.psycopg.org/docs/\">https://www.psycopg.org/docs/</a></p>\n<h3>Installing psycopg2 and connecting to the\u00a0database</h3>\n<p>To use psycopg2, we first need to install\u00a0it:</p>\n<pre>pip install psycopg2</pre>\n<p>Then, we need to connect to the database using the connect() function\u00a0(<a href=\"https://www.psycopg.org/docs/connection.html\">docs</a>):</p>\n<pre>conn = psycopg2.connect(\u201cdbname=test user=postgres password=secret\u201d)</pre>\n<p>The basic connection parameters are:</p>\n<pre><em>\u00b7 dbname \u2014 the database name</em></pre>\n<pre><em>\u00b7 user \u2014 user name used to authenticate</em></pre>\n<pre><em>\u00b7 password \u2014 password used to authenticate</em></pre>\n<pre><em>\u00b7 host \u2014 database host address</em></pre>\n<pre><em>\u00b7 port \u2014 connection port number (defaults to 5432 if not provided)</em></pre>\n<p>After creating the connection, we need to open a cursor to perform database operations (<a href=\"https://www.psycopg.org/docs/connection.html#connection.cursor\">docs</a>):</p>\n<pre><strong>cur = conn.cursor()</strong></pre>\n<p>Then we need to execute a command using the cursor. We can do all sorts of commands (<strong>CREATE TABLE, INSERT INTO, SELECT\u2026</strong>). Here we are going to focus on using the cursor to fetch\u00a0data:</p>\n<pre><strong>cur.execute(\"SELECT * FROM test;\"):</strong> Execute the query;</pre>\n<pre><strong>cur.fetchone()</strong>: Fetch the next row of a query result set, returning a single tuple, or None when no more data is available;</pre>\n<pre><strong>cur.fetchmany([size=cursor.arraysize]):</strong> Fetch the next set of rows of a query result, returning a list of tuples.</pre>\n<pre><strong>cur.fetchall():</strong> Fetch all (remaining) rows of a query result, returning them as a list of tuples</pre>\n<p>If any changes were made to the database we need to\u00a0commit:</p>\n<pre>conn.commit()</pre>\n<p>When we are done, we need to close the communication with the database:</p>\n<pre>cur.close()</pre>\n<pre>conn.close()</pre>\n<h3>Wrapping all this up in a function for convenience:</h3>\n<p>First, let\u2019s say we want to connect to our work database and it requires authentication. To avoid writing our credentials directly in our notebook and exposing them if we need to upload our code into GitHub or share our notebook with a colleague after our analysis is done, we can create a dotenv file with our credentials, and load it into our connect function. Then we can add our dotenv file to gitignore and also directly share our notebook without having our credentials leaked.</p>\n<p>Here\u2019s how to do\u00a0it:</p>\n<p><strong>Create a\u00a0.env file and write our credentials on\u00a0it:</strong></p>\n<blockquote>Since I\u2019m using Linux, I use a simple \u201ctouch\u00a0.env\u201d command to create the file. On other OS, just create the file whichever way is easier for\u00a0you.</blockquote>\n<p>Then, open the file and write our credentials:</p>\n<pre>DB_USERNAME=name</pre>\n<pre>DB_PASSWORD=password</pre>\n<pre>DB_PATH=path_to_database(if it\u2019s on AWS for example, you can type in the address like: yourcompany-database-address.us-east.rds.amazonaws.com)</pre>\n<pre>DB_NAME=db_name</pre>\n<p>Save and close the\u00a0file.</p>\n<p>Then, we are going to use the dotenv python library to load our credentials:</p>\n<pre>pip install python-dotenv<br>from dotenv import load_dotenv<br>load_dotenv() # take environment variables from .env.</pre>\n<p>Now, for the function I use to connect to the database:</p>\n<pre>import psycopg2</pre>\n<pre>import os</pre>\n<pre>import sys</pre>\n<pre>def connect():<br><br>   \u201c\u201d\u201d Connect to database \u201c\u201d\u201d</pre>\n<pre>   conn = None</pre>\n<pre>   try:</pre>\n<pre>      print(\u2018Connecting\u2026\u2019)</pre>\n<pre>      conn = psycopg2.connect(</pre>\n<pre>                   host=os.environ[\u2018DB_PATH\u2019],</pre>\n<pre>                   database=os.environ[\u2018DB_NAME\u2019],</pre>\n<pre>                   user=os.environ[\u2018DB_USERNAME\u2019],</pre>\n<pre>                   password=os.environ[\u2018DB_PASSWORD\u2019])</pre>\n<pre>    except (Exception, psycopg2.DatabaseError) as error:</pre>\n<pre>       print(error)</pre>\n<pre>       sys.exit(1)</pre>\n<pre>   print(\u201cAll good, Connection successful!\u201d)</pre>\n<pre>   return conn</pre>\n<p>And now we write a function to create a pandas DataFrame using a SELECT\u00a0query:</p>\n<pre>def sql_to_dataframe(conn, query, column_names):</pre>\n<pre>   \u201c\u201d\u201d <br>   Import data from a PostgreSQL database using a SELECT query <br>   \u201c\u201d\u201d</pre>\n<pre>   cursor = conn.cursor()</pre>\n<pre>   try:</pre>\n<pre>      cursor.execute(query)</pre>\n<pre>   except (Exception, psycopg2.DatabaseError) as error:</pre>\n<pre>      print(\u201cError: %s\u201d % error)</pre>\n<pre>   cursor.close()</pre>\n<pre>   return 1</pre>\n<pre>   # The execute returns a list of tuples:</pre>\n<pre>   tuples_list = cursor.fetchall()</pre>\n<pre>   cursor.close()</pre>\n<pre>   # Now we need to transform the list into a pandas DataFrame:</pre>\n<pre>   df = pd.DataFrame(tuples_list, columns=column_names)</pre>\n<pre>   return df</pre>\n<p>For better convenience, I also recommend writing our functions into a\u00a0.py file so we can reuse it whenever we\u00a0want.</p>\n<h3>Loading information from the database as a pandas DataFrame</h3>\n<p>Now that we have everything we need, let\u2019s finally put it all together and load our DataFrame from the database in our Jupyter notebook. We\u2019re going to do that by importing Pandas, psycopg2, and our custom-made function for connecting and loading the data. Here\u2019s the whole\u00a0code:</p>\n<pre>#imports<br>import pandas as pd<br>import NumPy as np<br>import os<br>import psycopg2<br>from dotenv import load_dotenv<br>from functions import sql_to_dataframe, connect<br>load_dotenv()</pre>\n<pre><em>#creating a query variable to store our query to pass into the function</em></pre>\n<pre>query = \u201c\u201d\u201d SELECT column1, <br>                   column2, <br>                   column3 <br>            FROM database<br>        \u201d\u201d\u201d<br><em>#creating a list with columns names to pass into the function</em></pre>\n<pre>column_names = [\u2018column1\u2019,\u2018column2\u2019, \u2018column3\u2019]</pre>\n<pre><em>#opening the connection</em></pre>\n<pre>conn = connect()</pre>\n<pre><em>#loading our dataframe</em></pre>\n<pre>df = sql_to_dataframe(conn, query, column_names)</pre>\n<pre><em>#closing the connection</em></pre>\n<pre>conn.close()</pre>\n<pre># Let\u2019s see if we loaded the df successfully</pre>\n<pre>df.head()</pre>\n<h3>Conclusion</h3>\n<p>Finally, we\u2019re\u00a0done!</p>\n<p>In this tutorial, you learned how to import data from a PostgreSQL database to a pandas DataFrame, while also keeping your credentials safe and writing a useful function that you can reuse in the\u00a0future!</p>\n<p>Thanks for\u00a0reading!</p>\n<h3>Sources:</h3>\n<p><a href=\"https://www.psycopg.org/docs/connection.html#connection.cursor\">https://www.psycopg.org/docs/connection.html</a></p>\n<p><a href=\"https://www.postgresqltutorial.com/postgresql-python/connect/\">https://www.postgresqltutorial.com/postgresql-python/connect/</a></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=5f4bffcd8bb2\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*vE2qMiigkkMGdNf1vqNHbw.jpeg\"><figcaption>Photo by <a href=\"https://unsplash.com/@purzlbaum\">Claudio Schwarz</a> on\u00a0<a href=\"https://unsplash.com/\">unsplash</a></figcaption></figure><h3>In this article, we\u2019ll go over how to create a pandas DataFrame using a simple connection and query to fetch data from a PostgreSQL database that requires authentication.</h3>\n<p>Most people that work with data know that SQL is great for working with tabular\u00a0data.</p>\n<p>Depending on what company you are working for, you may not have access to no-code BI tools like Tableau or PowerBI. That means that when doing your analysis, you will probably have to perform all transformations &amp; aggregations inside your DBMS and export data to a\u00a0.csv file and do the visualizations in Excel or Google Sheets. But if you know Python, you\u2019ll probably want to work with the dataset on Jupyter notebook with Pandas,\u00a0right?</p>\n<p>To work with PostgreSQL databases in Python I use <a href=\"https://pypi.org/project/psycopg2/\"><strong>psycopg2</strong></a>, which is the most popular PostgreSQL database adapter for\u00a0Python.</p>\n<p>The documentation for psycopg2 is found here: <a href=\"https://www.psycopg.org/docs/\">https://www.psycopg.org/docs/</a></p>\n<h3>Installing psycopg2 and connecting to the\u00a0database</h3>\n<p>To use psycopg2, we first need to install\u00a0it:</p>\n<pre>pip install psycopg2</pre>\n<p>Then, we need to connect to the database using the connect() function\u00a0(<a href=\"https://www.psycopg.org/docs/connection.html\">docs</a>):</p>\n<pre>conn = psycopg2.connect(\u201cdbname=test user=postgres password=secret\u201d)</pre>\n<p>The basic connection parameters are:</p>\n<pre><em>\u00b7 dbname \u2014 the database name</em></pre>\n<pre><em>\u00b7 user \u2014 user name used to authenticate</em></pre>\n<pre><em>\u00b7 password \u2014 password used to authenticate</em></pre>\n<pre><em>\u00b7 host \u2014 database host address</em></pre>\n<pre><em>\u00b7 port \u2014 connection port number (defaults to 5432 if not provided)</em></pre>\n<p>After creating the connection, we need to open a cursor to perform database operations (<a href=\"https://www.psycopg.org/docs/connection.html#connection.cursor\">docs</a>):</p>\n<pre><strong>cur = conn.cursor()</strong></pre>\n<p>Then we need to execute a command using the cursor. We can do all sorts of commands (<strong>CREATE TABLE, INSERT INTO, SELECT\u2026</strong>). Here we are going to focus on using the cursor to fetch\u00a0data:</p>\n<pre><strong>cur.execute(\"SELECT * FROM test;\"):</strong> Execute the query;</pre>\n<pre><strong>cur.fetchone()</strong>: Fetch the next row of a query result set, returning a single tuple, or None when no more data is available;</pre>\n<pre><strong>cur.fetchmany([size=cursor.arraysize]):</strong> Fetch the next set of rows of a query result, returning a list of tuples.</pre>\n<pre><strong>cur.fetchall():</strong> Fetch all (remaining) rows of a query result, returning them as a list of tuples</pre>\n<p>If any changes were made to the database we need to\u00a0commit:</p>\n<pre>conn.commit()</pre>\n<p>When we are done, we need to close the communication with the database:</p>\n<pre>cur.close()</pre>\n<pre>conn.close()</pre>\n<h3>Wrapping all this up in a function for convenience:</h3>\n<p>First, let\u2019s say we want to connect to our work database and it requires authentication. To avoid writing our credentials directly in our notebook and exposing them if we need to upload our code into GitHub or share our notebook with a colleague after our analysis is done, we can create a dotenv file with our credentials, and load it into our connect function. Then we can add our dotenv file to gitignore and also directly share our notebook without having our credentials leaked.</p>\n<p>Here\u2019s how to do\u00a0it:</p>\n<p><strong>Create a\u00a0.env file and write our credentials on\u00a0it:</strong></p>\n<blockquote>Since I\u2019m using Linux, I use a simple \u201ctouch\u00a0.env\u201d command to create the file. On other OS, just create the file whichever way is easier for\u00a0you.</blockquote>\n<p>Then, open the file and write our credentials:</p>\n<pre>DB_USERNAME=name</pre>\n<pre>DB_PASSWORD=password</pre>\n<pre>DB_PATH=path_to_database(if it\u2019s on AWS for example, you can type in the address like: yourcompany-database-address.us-east.rds.amazonaws.com)</pre>\n<pre>DB_NAME=db_name</pre>\n<p>Save and close the\u00a0file.</p>\n<p>Then, we are going to use the dotenv python library to load our credentials:</p>\n<pre>pip install python-dotenv<br>from dotenv import load_dotenv<br>load_dotenv() # take environment variables from .env.</pre>\n<p>Now, for the function I use to connect to the database:</p>\n<pre>import psycopg2</pre>\n<pre>import os</pre>\n<pre>import sys</pre>\n<pre>def connect():<br><br>   \u201c\u201d\u201d Connect to database \u201c\u201d\u201d</pre>\n<pre>   conn = None</pre>\n<pre>   try:</pre>\n<pre>      print(\u2018Connecting\u2026\u2019)</pre>\n<pre>      conn = psycopg2.connect(</pre>\n<pre>                   host=os.environ[\u2018DB_PATH\u2019],</pre>\n<pre>                   database=os.environ[\u2018DB_NAME\u2019],</pre>\n<pre>                   user=os.environ[\u2018DB_USERNAME\u2019],</pre>\n<pre>                   password=os.environ[\u2018DB_PASSWORD\u2019])</pre>\n<pre>    except (Exception, psycopg2.DatabaseError) as error:</pre>\n<pre>       print(error)</pre>\n<pre>       sys.exit(1)</pre>\n<pre>   print(\u201cAll good, Connection successful!\u201d)</pre>\n<pre>   return conn</pre>\n<p>And now we write a function to create a pandas DataFrame using a SELECT\u00a0query:</p>\n<pre>def sql_to_dataframe(conn, query, column_names):</pre>\n<pre>   \u201c\u201d\u201d <br>   Import data from a PostgreSQL database using a SELECT query <br>   \u201c\u201d\u201d</pre>\n<pre>   cursor = conn.cursor()</pre>\n<pre>   try:</pre>\n<pre>      cursor.execute(query)</pre>\n<pre>   except (Exception, psycopg2.DatabaseError) as error:</pre>\n<pre>      print(\u201cError: %s\u201d % error)</pre>\n<pre>   cursor.close()</pre>\n<pre>   return 1</pre>\n<pre>   # The execute returns a list of tuples:</pre>\n<pre>   tuples_list = cursor.fetchall()</pre>\n<pre>   cursor.close()</pre>\n<pre>   # Now we need to transform the list into a pandas DataFrame:</pre>\n<pre>   df = pd.DataFrame(tuples_list, columns=column_names)</pre>\n<pre>   return df</pre>\n<p>For better convenience, I also recommend writing our functions into a\u00a0.py file so we can reuse it whenever we\u00a0want.</p>\n<h3>Loading information from the database as a pandas DataFrame</h3>\n<p>Now that we have everything we need, let\u2019s finally put it all together and load our DataFrame from the database in our Jupyter notebook. We\u2019re going to do that by importing Pandas, psycopg2, and our custom-made function for connecting and loading the data. Here\u2019s the whole\u00a0code:</p>\n<pre>#imports<br>import pandas as pd<br>import NumPy as np<br>import os<br>import psycopg2<br>from dotenv import load_dotenv<br>from functions import sql_to_dataframe, connect<br>load_dotenv()</pre>\n<pre><em>#creating a query variable to store our query to pass into the function</em></pre>\n<pre>query = \u201c\u201d\u201d SELECT column1, <br>                   column2, <br>                   column3 <br>            FROM database<br>        \u201d\u201d\u201d<br><em>#creating a list with columns names to pass into the function</em></pre>\n<pre>column_names = [\u2018column1\u2019,\u2018column2\u2019, \u2018column3\u2019]</pre>\n<pre><em>#opening the connection</em></pre>\n<pre>conn = connect()</pre>\n<pre><em>#loading our dataframe</em></pre>\n<pre>df = sql_to_dataframe(conn, query, column_names)</pre>\n<pre><em>#closing the connection</em></pre>\n<pre>conn.close()</pre>\n<pre># Let\u2019s see if we loaded the df successfully</pre>\n<pre>df.head()</pre>\n<h3>Conclusion</h3>\n<p>Finally, we\u2019re\u00a0done!</p>\n<p>In this tutorial, you learned how to import data from a PostgreSQL database to a pandas DataFrame, while also keeping your credentials safe and writing a useful function that you can reuse in the\u00a0future!</p>\n<p>Thanks for\u00a0reading!</p>\n<h3>Sources:</h3>\n<p><a href=\"https://www.psycopg.org/docs/connection.html#connection.cursor\">https://www.psycopg.org/docs/connection.html</a></p>\n<p><a href=\"https://www.postgresqltutorial.com/postgresql-python/connect/\">https://www.postgresqltutorial.com/postgresql-python/connect/</a></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=5f4bffcd8bb2\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["pandas","postgres","data-science","pandas-dataframe","python"]}]}