{"status":"ok","feed":{"url":"https://medium.com/feed/@alestamm","title":"Stories by Alexandre Stamm on Medium","link":"https://medium.com/@alestamm?source=rss-bdc9fb848ad5------2","author":"","description":"Stories by Alexandre Stamm on Medium","image":"https://cdn-images-1.medium.com/fit/c/150/150/1*0kAEJ3sJCwORPCSXAvTMJA.jpeg"},"items":[{"title":"Data visualization with Python &amp; Plotly","pubDate":"2022-07-24 23:23:07","link":"https://medium.com/@alestamm/data-visualization-with-python-plotly-770bfd7e5854?source=rss-bdc9fb848ad5------2","guid":"https://medium.com/p/770bfd7e5854","author":"Alexandre Stamm","thumbnail":"https://cdn-images-1.medium.com/max/1024/0*JpZNn_BdJLDyw039","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*JpZNn_BdJLDyw039\"><figcaption>Photo by <a href=\"https://unsplash.com/@goumbik?utm_source=medium&amp;utm_medium=referral\">Lukas Blazek</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><h3>How to make beautiful charts &amp; dashboards using\u00a0Plotly</h3>\n<p>Hello everyone!</p>\n<p>Lately, I\u2019ve been using a lot more <a href=\"https://plotly.com/\">Plotly</a> than <a href=\"https://seaborn.pydata.org/\">Seaborn</a> for my Data Visualizations, for a few\u00a0reasons:</p>\n<p>\u00b7 Plotly charts are interactive, and you can make dashboards easily;</p>\n<p>\u00b7 Some things are much simpler to do in Plotly compared to Seaborn, like putting legend numbers in a line\u00a0plot;</p>\n<p>\u00b7 Usually when you provide an invalid parameter to a Plotly function/object, it returns a useful error message, with suggestions on what you might have\u00a0missed;</p>\n<p>\u00b7 Also, I think the default style of Plotly is better than the default style of\u00a0Seaborn.</p>\n<p>It\u2019s not like Seaborn doesn\u2019t have its advantages either, we all know how powerful and complete the Seaborn library is for data visualization. But lately, when I just need to plot some simple and beautiful charts, Plotly has been my go-to\u00a0library.</p>\n<p>That being said, today I\u2019m going to write some general tips &amp; tricks about Plotly, and some examples of what kind of visualizations we can make with this amazing\u00a0library!</p>\n<p>First, if you would like to learn Plotly more in-depth, from how the figure data works, to publishing a Dashboard app using <a href=\"https://dash.plotly.com/\">Dash</a>, I suggest reading the <a href=\"https://plotly.com/python/\">documentation</a>, which is very well written and complete.</p>\n<p>Now let\u2019s get\u00a0started!</p>\n<p>First, there are two main ways to plot graphs in Plotly, using <a href=\"https://plotly.com/python/plotly-express/\">Plotly Express</a> or <a href=\"https://plotly.com/python/graph-objects/\">Plotly Go</a> (Graph objects).</p>\n<p>I mostly use Plotly express when I just need some simple graph plotted, and use Go when I need more customization in my\u00a0graphs.</p>\n<p>Plotting charts using Plotly Express is very simple, here are some examples:</p>\n<h4>\n<a href=\"https://plotly.com/python/bar-charts/\">Bar charts</a>:</h4>\n<p>To plot a bar chart with Plotly Express, first, we import a sample dataframe from the library, and then call the px.bar argument:</p>\n<pre>import plotly.express as px<br>data_canada = px.data.gapminder().query(\"country == 'Canada'\")<br>fig = px.bar(data_canada, x='year', y='pop')<br>fig.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/880/1*ue0hT2jri-oeD8BYGRjhFQ.png\"></figure><p>Now, say we want to add legend text inside our bars, it\u2019s as simple as adding the <strong>text_auto=True</strong> argument.</p>\n<pre>import plotly.express as px<br>df = px.data.medals_long()<br><br>fig = px.bar(df, x=\"medal\", y=\"count\", color=\"nation\", text_auto=<strong>True</strong>)<br>fig.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/887/1*yzwrS7mqQjq0reNXlVVjKw.png\"></figure><h4>\n<a href=\"https://plotly.com/python/line-charts/\">Line charts</a>:</h4>\n<p>Just like with our bar charts, we\u2019re going to import a sample df and use px.line to plot our\u00a0chart:</p>\n<pre>import plotly.express as px<br><br>df = px.data.gapminder().query(\"country=='Canada'\")<br>fig = px.line(df, x=\"year\", y=\"lifeExp\", title='Life expectancy in Canada')<br>fig.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/864/1*KYG4a9MlMajULmnJW0qYhA.png\"></figure><p>Now, to show text legends on our line chart, the code is a little different from the bar chart, but it\u2019s also very simple, we just have to specify where Plotly should get our numbers from (in this case, the \u201cyear\u201d column), and it\u2019s also good to use update_traces to specify where we want our text to\u00a0show:</p>\n<pre>import plotly.express as px<br><br>df = px.data.gapminder().query(\"country in ['Canada', 'Botswana']\")<br><br>fig = px.line(df, x=\"lifeExp\", y=\"gdpPercap\", color=\"country\", text=\"year\")<br>fig.update_traces(textposition=\"bottom right\")<br>fig.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/923/1*PdISTUlo0YaMGxpE5SO5Kw.png\"></figure><p>You can also make all other sorts of charts, like <a href=\"https://plotly.com/python/histograms/\">histograms</a>, <a href=\"https://plotly.com/python/distplot/\">distplots</a>, <a href=\"https://plotly.com/python/box-plots/\">boxplots</a>, <a href=\"https://plotly.com/python/heatmaps/\">heatmaps</a>, <a href=\"https://plotly.com/python/waterfall-charts/\">waterfall</a>\u2026 You can find a list of all kinds of charts you can plot\u00a0<a href=\"https://plotly.com/python/\">here</a>.</p>\n<h4>Using Plotly Graph Objects for more customizable graphics:</h4>\n<p>Now let\u2019s try something harder!</p>\n<p>Say we want to plot a Bar &amp; Line chart, but using a secondary Y axis for our line chart. Here\u2019s where the customization of Plotly Go comes in\u00a0hand!</p>\n<p>First, let\u2019s create a sample dataframe, with data from visitors in a public\u00a0park.</p>\n<a href=\"https://medium.com/media/8409f19b4a2683eba236cb5fe14fe6c7/href\">https://medium.com/media/8409f19b4a2683eba236cb5fe14fe6c7/href</a><p>Let\u2019s code our chart! I\u2019m going to explain how to do it step-by-step in the code\u00a0below:</p>\n<a href=\"https://medium.com/media/886a8491594eff9dc5b98fdd6cc77a66/href\">https://medium.com/media/886a8491594eff9dc5b98fdd6cc77a66/href</a><p>Here\u2019s how our chart looks\u00a0like:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*zHkRQe9HrMCqxeee4px0FA.png\"></figure><p>And we\u2019re\u00a0done!</p>\n<p>Since we did our chart with Plotly, we can also use Dash to turn it into a dashboard/interactive chart!</p>\n<p>There\u2019s a very useful guide on how to deploy your dashboard for free using Heroku that you can find\u00a0<a href=\"https://dash.plotly.com/deployment\">here</a>.</p>\n<p>That\u2019s it for today! If you liked this guide, please leave your comment and follow me for more Data Visualization tips/tutorials!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=770bfd7e5854\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*JpZNn_BdJLDyw039\"><figcaption>Photo by <a href=\"https://unsplash.com/@goumbik?utm_source=medium&amp;utm_medium=referral\">Lukas Blazek</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><h3>How to make beautiful charts &amp; dashboards using\u00a0Plotly</h3>\n<p>Hello everyone!</p>\n<p>Lately, I\u2019ve been using a lot more <a href=\"https://plotly.com/\">Plotly</a> than <a href=\"https://seaborn.pydata.org/\">Seaborn</a> for my Data Visualizations, for a few\u00a0reasons:</p>\n<p>\u00b7 Plotly charts are interactive, and you can make dashboards easily;</p>\n<p>\u00b7 Some things are much simpler to do in Plotly compared to Seaborn, like putting legend numbers in a line\u00a0plot;</p>\n<p>\u00b7 Usually when you provide an invalid parameter to a Plotly function/object, it returns a useful error message, with suggestions on what you might have\u00a0missed;</p>\n<p>\u00b7 Also, I think the default style of Plotly is better than the default style of\u00a0Seaborn.</p>\n<p>It\u2019s not like Seaborn doesn\u2019t have its advantages either, we all know how powerful and complete the Seaborn library is for data visualization. But lately, when I just need to plot some simple and beautiful charts, Plotly has been my go-to\u00a0library.</p>\n<p>That being said, today I\u2019m going to write some general tips &amp; tricks about Plotly, and some examples of what kind of visualizations we can make with this amazing\u00a0library!</p>\n<p>First, if you would like to learn Plotly more in-depth, from how the figure data works, to publishing a Dashboard app using <a href=\"https://dash.plotly.com/\">Dash</a>, I suggest reading the <a href=\"https://plotly.com/python/\">documentation</a>, which is very well written and complete.</p>\n<p>Now let\u2019s get\u00a0started!</p>\n<p>First, there are two main ways to plot graphs in Plotly, using <a href=\"https://plotly.com/python/plotly-express/\">Plotly Express</a> or <a href=\"https://plotly.com/python/graph-objects/\">Plotly Go</a> (Graph objects).</p>\n<p>I mostly use Plotly express when I just need some simple graph plotted, and use Go when I need more customization in my\u00a0graphs.</p>\n<p>Plotting charts using Plotly Express is very simple, here are some examples:</p>\n<h4>\n<a href=\"https://plotly.com/python/bar-charts/\">Bar charts</a>:</h4>\n<p>To plot a bar chart with Plotly Express, first, we import a sample dataframe from the library, and then call the px.bar argument:</p>\n<pre>import plotly.express as px<br>data_canada = px.data.gapminder().query(\"country == 'Canada'\")<br>fig = px.bar(data_canada, x='year', y='pop')<br>fig.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/880/1*ue0hT2jri-oeD8BYGRjhFQ.png\"></figure><p>Now, say we want to add legend text inside our bars, it\u2019s as simple as adding the <strong>text_auto=True</strong> argument.</p>\n<pre>import plotly.express as px<br>df = px.data.medals_long()<br><br>fig = px.bar(df, x=\"medal\", y=\"count\", color=\"nation\", text_auto=<strong>True</strong>)<br>fig.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/887/1*yzwrS7mqQjq0reNXlVVjKw.png\"></figure><h4>\n<a href=\"https://plotly.com/python/line-charts/\">Line charts</a>:</h4>\n<p>Just like with our bar charts, we\u2019re going to import a sample df and use px.line to plot our\u00a0chart:</p>\n<pre>import plotly.express as px<br><br>df = px.data.gapminder().query(\"country=='Canada'\")<br>fig = px.line(df, x=\"year\", y=\"lifeExp\", title='Life expectancy in Canada')<br>fig.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/864/1*KYG4a9MlMajULmnJW0qYhA.png\"></figure><p>Now, to show text legends on our line chart, the code is a little different from the bar chart, but it\u2019s also very simple, we just have to specify where Plotly should get our numbers from (in this case, the \u201cyear\u201d column), and it\u2019s also good to use update_traces to specify where we want our text to\u00a0show:</p>\n<pre>import plotly.express as px<br><br>df = px.data.gapminder().query(\"country in ['Canada', 'Botswana']\")<br><br>fig = px.line(df, x=\"lifeExp\", y=\"gdpPercap\", color=\"country\", text=\"year\")<br>fig.update_traces(textposition=\"bottom right\")<br>fig.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/923/1*PdISTUlo0YaMGxpE5SO5Kw.png\"></figure><p>You can also make all other sorts of charts, like <a href=\"https://plotly.com/python/histograms/\">histograms</a>, <a href=\"https://plotly.com/python/distplot/\">distplots</a>, <a href=\"https://plotly.com/python/box-plots/\">boxplots</a>, <a href=\"https://plotly.com/python/heatmaps/\">heatmaps</a>, <a href=\"https://plotly.com/python/waterfall-charts/\">waterfall</a>\u2026 You can find a list of all kinds of charts you can plot\u00a0<a href=\"https://plotly.com/python/\">here</a>.</p>\n<h4>Using Plotly Graph Objects for more customizable graphics:</h4>\n<p>Now let\u2019s try something harder!</p>\n<p>Say we want to plot a Bar &amp; Line chart, but using a secondary Y axis for our line chart. Here\u2019s where the customization of Plotly Go comes in\u00a0hand!</p>\n<p>First, let\u2019s create a sample dataframe, with data from visitors in a public\u00a0park.</p>\n<a href=\"https://medium.com/media/8409f19b4a2683eba236cb5fe14fe6c7/href\">https://medium.com/media/8409f19b4a2683eba236cb5fe14fe6c7/href</a><p>Let\u2019s code our chart! I\u2019m going to explain how to do it step-by-step in the code\u00a0below:</p>\n<a href=\"https://medium.com/media/886a8491594eff9dc5b98fdd6cc77a66/href\">https://medium.com/media/886a8491594eff9dc5b98fdd6cc77a66/href</a><p>Here\u2019s how our chart looks\u00a0like:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*zHkRQe9HrMCqxeee4px0FA.png\"></figure><p>And we\u2019re\u00a0done!</p>\n<p>Since we did our chart with Plotly, we can also use Dash to turn it into a dashboard/interactive chart!</p>\n<p>There\u2019s a very useful guide on how to deploy your dashboard for free using Heroku that you can find\u00a0<a href=\"https://dash.plotly.com/deployment\">here</a>.</p>\n<p>That\u2019s it for today! If you liked this guide, please leave your comment and follow me for more Data Visualization tips/tutorials!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=770bfd7e5854\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["data-science","plotly","pandas","data-visualization","python"]},{"title":"Checking and cleaning data in Python with Pandas","pubDate":"2022-06-19 23:33:27","link":"https://medium.com/@alestamm/checking-and-cleaning-data-in-python-with-pandas-c955f45f0f7?source=rss-bdc9fb848ad5------2","guid":"https://medium.com/p/c955f45f0f7","author":"Alexandre Stamm","thumbnail":"https://cdn-images-1.medium.com/max/1024/0*N3e_Ft7TrzAVcZp0","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*N3e_Ft7TrzAVcZp0\"><figcaption>Photo by <a href=\"https://unsplash.com/@towfiqu999999?utm_source=medium&amp;utm_medium=referral\">Towfiqu barbhuiya</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>Hello everyone! In my previous articles, we learned about how to <a href=\"https://medium.com/@alestamm/how-to-load-data-into-a-pandas-dataframe-2f1a3586e415\">load into a Pandas DataFrame</a>, and how to <a href=\"https://medium.com/@alestamm/importing-data-from-a-postgresql-database-to-a-pandas-dataframe-5f4bffcd8bb2\">Import data from a PostgreSQL database to a Pandas DataFrame</a>.</p>\n<p>You may have heard many times that Data scientists spend most of their time cleaning and organizing data. That may change depending on what type of data you work on, but regardless of how messy the data of your company/project is, organizing our data is one of the most important parts of a Data Scientist/Data Analyst\u00a0job.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*hCYX87DYpXJ5pzby\"><figcaption>Photo by <a href=\"https://unsplash.com/@dav420?utm_source=medium&amp;utm_medium=referral\">David Pupaza</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>According to a <a href=\"https://hbr.org/2016/09/bad-data-costs-the-u-s-3-trillion-per-year\">report released by IBM in 2016</a>, the cost of bad data to U.S. businesses and organizations was around 3.1 trillion dollars per year, and a big source of bad data is, guess what? Human\u00a0error!</p>\n<p>So today we\u2019re going to dive a little deeper into our data and use Pandas and some Python functions to clean and organize our\u00a0data.</p>\n<p>First, when working on a dataset, before you start cleaning and organizing your data, it\u2019s good practice to get a first look at what we\u2019re dealing\u00a0with.</p>\n<p>To do this, I generally use the following functions, and I\u2019m going to pass over them quickly since they are really\u00a0basic:</p>\n<pre>import pandas as pd</pre>\n<pre># Print the first 5 rows (or whatever number you pass inside the parentheses) of the DataFrame<br>df.head()<br>df.tail() # same as .head() but returns the last rows</pre>\n<pre># Summarize the central tendency, dispersion, and shape of a dataset\u2019s distribution, excluding NaN values<br>df.describe()</pre>\n<pre># Checking the dimensionality of the DataFrame<br>df.shape</pre>\n<pre># Checking data types of the DataFrame<br>df.dtypes</pre>\n<pre># Printing a concise summary of a DataFrame<br>df.info()</pre>\n<p>From here, I usually plot some graphics to get a feeling of what we\u2019re dealing with. Graphics are a good way to check the distribution and possible outliers of a dataset. A good library to plot data in python is <a href=\"https://seaborn.pydata.org/index.html\">Seaborn</a>. Seaborn is a Python data visualization library based on <a href=\"https://matplotlib.org/\">matplotlib</a>. It provides a high-level interface for drawing attractive and informative statistical graphics.</p>\n<p>The canonical import of\u00a0seaborn:</p>\n<pre>import seaborn as sns</pre>\n<p>We can use <a href=\"https://seaborn.pydata.org/generated/seaborn.histplot.html\">sns.histplot()</a> to plot a histogram and check distribution, <a href=\"https://seaborn.pydata.org/generated/seaborn.scatterplot.html#seaborn.scatterplot\">sns.scatterplot() </a>and <a href=\"https://seaborn.pydata.org/generated/seaborn.lineplot.html#seaborn.lineplot\">sns.lineplot</a> to check for statistical relationships, or <a href=\"https://seaborn.pydata.org/generated/seaborn.boxplot.html#seaborn.boxplot\">sns.boxplot()</a>, <a href=\"https://seaborn.pydata.org/generated/seaborn.barplot.html#seaborn.barplot\">sns.barplot()</a>, and <a href=\"https://seaborn.pydata.org/generated/seaborn.countplot.html#seaborn.countplot\">sns.countplot()</a> for categorical variables.</p>\n<p>Ok! After we\u2019re done with all that, we can start cleaning our data and preparing our analysis.</p>\n<p>First, everyone that works as a Data/Business Analyst or Data Scientist knows that a lot of problems when dealing with data are caused by null\u00a0values.</p>\n<p>Pandas has a lot of useful functions to deal with Na\u2019s values. The most common ones\u00a0are:</p>\n<p><a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html\">.dropna()</a>: Remove missing\u00a0values.</p>\n<p><a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html\">.filllna()</a>: Fill NA/NaN values using the specified method.</p>\n<p>Using\u00a0.fillna(), you can pass the value directly to the function:</p>\n<pre>df.fillna(\u2018None\u2019)<br>or<br>df.fillna(0)<br>or<br>df.fillna('2022-06-01')</pre>\n<p>Or you can use forward(ffill) and back fill(bfill) methods:</p>\n<pre># Ffill propagates the last valid observation forward to the next valid one.<br>df.fillna(method=ffill)</pre>\n<pre># Bfill uses the next valid observation to fill the gap.<br>df.fillna(method=\u201dbfill\u201d):</pre>\n<p>After dealing with nulls, I usually check the data types of our dataset so we have every column with its correct type. Sometimes dates and numbers can come as strings, and it\u2019s better to deal with this right at the beginning.</p>\n<p>For dealing with data types, I mostly use the following functions:</p>\n<p><a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html\">.astype()</a>:</p>\n<p>Cast a pandas object to a specified dtype.</p>\n<p><a href=\"https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html\">pd.to_datetime()</a>:</p>\n<p>Convert argument to datetime.</p>\n<p>Now, after dealing with nulls and wrong data types, we usually begin to dive deeper into the data itself. Here we have a lot of options on how to clean/organize our\u00a0data.</p>\n<p>Sometimes it\u2019s useful when dealing with categorical data to create clusters so we can aggregate our data in a way that makes\u00a0sense.</p>\n<p>One of these days I was dealing with some data from Google Analytics and had to narrow down our access sources for the analysis. When doing something like this, you can create a custom function and apply it to a column of your DataFrame. Here\u2019s an\u00a0example:</p>\n<pre>def clustering(x):</pre>\n<pre>   if 'google / cpc' in x or 'Google / CPC' in x or 'adwords' in x:</pre>\n<pre>      return 'Google'</pre>\n<pre>   elif 'fb' in x or 'facebook' in x:</pre>\n<pre>      return 'Facebook'</pre>\n<pre>``` Here we're going to use a list created beforehand and #check our dataframe for instances of our list. Since filtering strings like this is case-sensitive, we use .lower() to convert all strings to lowercase ```</pre>\n<pre>   elif any([k.lower() in x.lower() for k in list]):<br>      return 'In list'</pre>\n<pre>   elif 'insta' in x or 'ig' in x:</pre>\n<pre>      return 'Instagram'<br>   else: <br>      return 'Others'</pre>\n<p>Now that we have our function, we can use\u00a0<a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html\">.apply</a> to apply it to our DataFrame/column:</p>\n<pre>df['column'] = df['column'].apply(lambda x: clustering(x))</pre>\n<p>And just like that, our function will go over each value on our column and apply the clustering we\u00a0defined.</p>\n<p>From here, I usually start organizing, grouping, and feature engineering our data, and I\u2019m going to write about those steps in the near\u00a0future.</p>\n<p>If you read this far, thank you for your time and I hope I\u2019ve managed to give you some insights on how to start dealing with data before doing your analysis!</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*PEbapZA-1ZLZ9dkd\"><figcaption>Photo by <a href=\"https://unsplash.com/@craft_ear?utm_source=medium&amp;utm_medium=referral\">Jan Tinneberg</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c955f45f0f7\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*N3e_Ft7TrzAVcZp0\"><figcaption>Photo by <a href=\"https://unsplash.com/@towfiqu999999?utm_source=medium&amp;utm_medium=referral\">Towfiqu barbhuiya</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>Hello everyone! In my previous articles, we learned about how to <a href=\"https://medium.com/@alestamm/how-to-load-data-into-a-pandas-dataframe-2f1a3586e415\">load into a Pandas DataFrame</a>, and how to <a href=\"https://medium.com/@alestamm/importing-data-from-a-postgresql-database-to-a-pandas-dataframe-5f4bffcd8bb2\">Import data from a PostgreSQL database to a Pandas DataFrame</a>.</p>\n<p>You may have heard many times that Data scientists spend most of their time cleaning and organizing data. That may change depending on what type of data you work on, but regardless of how messy the data of your company/project is, organizing our data is one of the most important parts of a Data Scientist/Data Analyst\u00a0job.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*hCYX87DYpXJ5pzby\"><figcaption>Photo by <a href=\"https://unsplash.com/@dav420?utm_source=medium&amp;utm_medium=referral\">David Pupaza</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>According to a <a href=\"https://hbr.org/2016/09/bad-data-costs-the-u-s-3-trillion-per-year\">report released by IBM in 2016</a>, the cost of bad data to U.S. businesses and organizations was around 3.1 trillion dollars per year, and a big source of bad data is, guess what? Human\u00a0error!</p>\n<p>So today we\u2019re going to dive a little deeper into our data and use Pandas and some Python functions to clean and organize our\u00a0data.</p>\n<p>First, when working on a dataset, before you start cleaning and organizing your data, it\u2019s good practice to get a first look at what we\u2019re dealing\u00a0with.</p>\n<p>To do this, I generally use the following functions, and I\u2019m going to pass over them quickly since they are really\u00a0basic:</p>\n<pre>import pandas as pd</pre>\n<pre># Print the first 5 rows (or whatever number you pass inside the parentheses) of the DataFrame<br>df.head()<br>df.tail() # same as .head() but returns the last rows</pre>\n<pre># Summarize the central tendency, dispersion, and shape of a dataset\u2019s distribution, excluding NaN values<br>df.describe()</pre>\n<pre># Checking the dimensionality of the DataFrame<br>df.shape</pre>\n<pre># Checking data types of the DataFrame<br>df.dtypes</pre>\n<pre># Printing a concise summary of a DataFrame<br>df.info()</pre>\n<p>From here, I usually plot some graphics to get a feeling of what we\u2019re dealing with. Graphics are a good way to check the distribution and possible outliers of a dataset. A good library to plot data in python is <a href=\"https://seaborn.pydata.org/index.html\">Seaborn</a>. Seaborn is a Python data visualization library based on <a href=\"https://matplotlib.org/\">matplotlib</a>. It provides a high-level interface for drawing attractive and informative statistical graphics.</p>\n<p>The canonical import of\u00a0seaborn:</p>\n<pre>import seaborn as sns</pre>\n<p>We can use <a href=\"https://seaborn.pydata.org/generated/seaborn.histplot.html\">sns.histplot()</a> to plot a histogram and check distribution, <a href=\"https://seaborn.pydata.org/generated/seaborn.scatterplot.html#seaborn.scatterplot\">sns.scatterplot() </a>and <a href=\"https://seaborn.pydata.org/generated/seaborn.lineplot.html#seaborn.lineplot\">sns.lineplot</a> to check for statistical relationships, or <a href=\"https://seaborn.pydata.org/generated/seaborn.boxplot.html#seaborn.boxplot\">sns.boxplot()</a>, <a href=\"https://seaborn.pydata.org/generated/seaborn.barplot.html#seaborn.barplot\">sns.barplot()</a>, and <a href=\"https://seaborn.pydata.org/generated/seaborn.countplot.html#seaborn.countplot\">sns.countplot()</a> for categorical variables.</p>\n<p>Ok! After we\u2019re done with all that, we can start cleaning our data and preparing our analysis.</p>\n<p>First, everyone that works as a Data/Business Analyst or Data Scientist knows that a lot of problems when dealing with data are caused by null\u00a0values.</p>\n<p>Pandas has a lot of useful functions to deal with Na\u2019s values. The most common ones\u00a0are:</p>\n<p><a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html\">.dropna()</a>: Remove missing\u00a0values.</p>\n<p><a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html\">.filllna()</a>: Fill NA/NaN values using the specified method.</p>\n<p>Using\u00a0.fillna(), you can pass the value directly to the function:</p>\n<pre>df.fillna(\u2018None\u2019)<br>or<br>df.fillna(0)<br>or<br>df.fillna('2022-06-01')</pre>\n<p>Or you can use forward(ffill) and back fill(bfill) methods:</p>\n<pre># Ffill propagates the last valid observation forward to the next valid one.<br>df.fillna(method=ffill)</pre>\n<pre># Bfill uses the next valid observation to fill the gap.<br>df.fillna(method=\u201dbfill\u201d):</pre>\n<p>After dealing with nulls, I usually check the data types of our dataset so we have every column with its correct type. Sometimes dates and numbers can come as strings, and it\u2019s better to deal with this right at the beginning.</p>\n<p>For dealing with data types, I mostly use the following functions:</p>\n<p><a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html\">.astype()</a>:</p>\n<p>Cast a pandas object to a specified dtype.</p>\n<p><a href=\"https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html\">pd.to_datetime()</a>:</p>\n<p>Convert argument to datetime.</p>\n<p>Now, after dealing with nulls and wrong data types, we usually begin to dive deeper into the data itself. Here we have a lot of options on how to clean/organize our\u00a0data.</p>\n<p>Sometimes it\u2019s useful when dealing with categorical data to create clusters so we can aggregate our data in a way that makes\u00a0sense.</p>\n<p>One of these days I was dealing with some data from Google Analytics and had to narrow down our access sources for the analysis. When doing something like this, you can create a custom function and apply it to a column of your DataFrame. Here\u2019s an\u00a0example:</p>\n<pre>def clustering(x):</pre>\n<pre>   if 'google / cpc' in x or 'Google / CPC' in x or 'adwords' in x:</pre>\n<pre>      return 'Google'</pre>\n<pre>   elif 'fb' in x or 'facebook' in x:</pre>\n<pre>      return 'Facebook'</pre>\n<pre>``` Here we're going to use a list created beforehand and #check our dataframe for instances of our list. Since filtering strings like this is case-sensitive, we use .lower() to convert all strings to lowercase ```</pre>\n<pre>   elif any([k.lower() in x.lower() for k in list]):<br>      return 'In list'</pre>\n<pre>   elif 'insta' in x or 'ig' in x:</pre>\n<pre>      return 'Instagram'<br>   else: <br>      return 'Others'</pre>\n<p>Now that we have our function, we can use\u00a0<a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html\">.apply</a> to apply it to our DataFrame/column:</p>\n<pre>df['column'] = df['column'].apply(lambda x: clustering(x))</pre>\n<p>And just like that, our function will go over each value on our column and apply the clustering we\u00a0defined.</p>\n<p>From here, I usually start organizing, grouping, and feature engineering our data, and I\u2019m going to write about those steps in the near\u00a0future.</p>\n<p>If you read this far, thank you for your time and I hope I\u2019ve managed to give you some insights on how to start dealing with data before doing your analysis!</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*PEbapZA-1ZLZ9dkd\"><figcaption>Photo by <a href=\"https://unsplash.com/@craft_ear?utm_source=medium&amp;utm_medium=referral\">Jan Tinneberg</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c955f45f0f7\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["python","pandas","data-science","data","data-visualization"]},{"title":"How to load data into a Pandas DataFrame","pubDate":"2022-06-16 20:33:16","link":"https://medium.com/@alestamm/how-to-load-data-into-a-pandas-dataframe-2f1a3586e415?source=rss-bdc9fb848ad5------2","guid":"https://medium.com/p/2f1a3586e415","author":"Alexandre Stamm","thumbnail":"https://cdn-images-1.medium.com/max/1024/0*X2Bfp-_R-oLBySt1","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*X2Bfp-_R-oLBySt1\"><figcaption>Photo by <a href=\"https://unsplash.com/@fantasyflip?utm_source=medium&amp;utm_medium=referral\">Philipp Katzenberger</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>In my next articles, I\u2019m going to write some tips &amp; tricks about Pandas, from basic to advanced!</p>\n<p>I\u2019m going to keep it short and refer to the documentation if you need a more detailed explanation of how things\u00a0work!</p>\n<p>Today we\u2019re going to start with the basics: How to load data into a Pandas DataFrame!</p>\n<h4><strong>Loading data into a Pandas DataFrame:</strong></h4>\n<p>In Pandas, you can create DataFrames using many different ways, like importing data from CSV, JSON, Excel, HTML, XML, and many other file types, or loading data from a SQL or Google BigQuery database.</p>\n<p>You can check all the functions for loading DataFrames on this page of the Pandas <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/io.html\">documentation</a>.</p>\n<p>In this tip, I\u2019m going to focus on the 2 main ways I load data into Pandas DataFrames: reading from CSV or connecting directly to an SQL database.</p>\n<h4>Importing data from an csv\u00a0file:</h4>\n<p>To import data from a csv file use <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html#:~:text=If%20True%20and%20parse_dates%20is,parsing%20speed%20by%205-10x.\">pd.read_csv()</a>:</p>\n<p>If your file is in the same folder as your notebook, the easiest way to use pd.read_csv() is just to pass the filename directly to the function:</p>\n<pre>df = pd.read_csv (\u2018file_name.csv\u2019)</pre>\n<p>In case your file is not in the same folder as your notebook, you pass the file path (absolute or relative) to the function:</p>\n<pre>#Absolute path:<br>df = pd.read_csv(\u2018C:/path/to/you/file/file_name.csv\u2019)</pre>\n<pre>#Relative path:<br>df = pd.read_csv(\u2018\u2026/ file_name.csv\u2019)</pre>\n<p>When passing just the file name to the function, Pandas will infer most of the information it needs, like separators, columns data types, and indexes. But you can also use the function parameters to better deal with the data you\u2019re going to\u00a0import.</p>\n<p>There are a series of parameters you can pass to <strong>read_csv</strong>, but the ones I use the most\u00a0are:</p>\n<blockquote>\n<strong>header:</strong> Row number(s) to use as the column names, and the start of the data. The default behavior is to infer the column\u00a0names.</blockquote>\n<blockquote>\n<strong>sep: </strong>Delimiter to use, default is a\u00a0comma.</blockquote>\n<blockquote>\n<strong>index_col: </strong>Column(s) to use as the row labels of the DataFrame.</blockquote>\n<blockquote>\n<strong>dtype: </strong>Data type for data or columns, when you want to explicitly inform the dtype of columns when importing,<strong> E.g.</strong> {\u2018a\u2019: np.float64}.</blockquote>\n<blockquote>\n<strong>na_values: </strong>Additional strings to recognize as NA/NaN. Say you are importing a CSV where there are \u201cNone\u201d values, you can pass na_values=[\u2018None\u2019] and pandas will convert them to NaN when importing.</blockquote>\n<blockquote>\n<strong>parse_dates and infer_datetime_format: </strong>Useful when dealing with date columns. To keep this short I suggest looking at the <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html#:~:text=If%20True%20and%20parse_dates%20is,parsing%20speed%20by%205-10x.\">documentation</a> if you have to use these parameters.</blockquote>\n<h4><strong>Importing data from an SQL Database:</strong></h4>\n<p>The two main ways I import data from SQL databases are:</p>\n<p><strong>Importing from PostgreSQL databases:</strong></p>\n<p>Follow my guide for <a href=\"https://medium.com/@alestamm/importing-data-from-a-postgresql-database-to-a-pandas-dataframe-5f4bffcd8bb2\">importing data from a PostgreSQL database to a Pandas DataFrame</a>.</p>\n<p><strong>Importing data from SQLite databases:</strong></p>\n<p>To import data from an SQLite Database, I use a really helpful library called <strong>sqlite3</strong>(<a href=\"https://docs.python.org/3/library/sqlite3.html#module-sqlite3\">docs</a>).</p>\n<p>You can read the documentation and also use the guide I made for <a href=\"https://medium.com/@alestamm/importing-data-from-a-postgresql-database-to-a-pandas-dataframe-5f4bffcd8bb2\">importing data from PostgreSQL database </a>as a reference since the functions are very\u00a0similar.</p>\n<h4>Aside from importing data, you can also create DataFrames using these functions:</h4>\n<p>Using pd.DataFrame.from_dict() and passing a dictionary:</p>\n<pre>data = {\u2018col_1\u2019: [3, 2, 1, 0], \u2018col_2\u2019: [\u2018a\u2019, \u2018b\u2019, \u2018c\u2019, \u2018d\u2019]}<br>pd.DataFrame.from_dict(data)</pre>\n<p>If you are not running your notebook on the cloud (like on <a href=\"https://colab.research.google.com/notebooks/intro.ipynb?hl=pt_BR\">Google Colab</a>), you can read the data directly from your clipboard using <strong>pd.read_clipboard().</strong></p>\n<p>You can also create an DataFrame from a list or numpy array using <strong>pd.DataFrame():</strong></p>\n<pre>pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])</pre>\n<pre>or</pre>\n<pre>pd.DataFrame(list)</pre>\n<p>That\u2019s all for\u00a0today!</p>\n<p>Follow me to read my upcoming tips for working with\u00a0Pandas!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=2f1a3586e415\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*X2Bfp-_R-oLBySt1\"><figcaption>Photo by <a href=\"https://unsplash.com/@fantasyflip?utm_source=medium&amp;utm_medium=referral\">Philipp Katzenberger</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>In my next articles, I\u2019m going to write some tips &amp; tricks about Pandas, from basic to advanced!</p>\n<p>I\u2019m going to keep it short and refer to the documentation if you need a more detailed explanation of how things\u00a0work!</p>\n<p>Today we\u2019re going to start with the basics: How to load data into a Pandas DataFrame!</p>\n<h4><strong>Loading data into a Pandas DataFrame:</strong></h4>\n<p>In Pandas, you can create DataFrames using many different ways, like importing data from CSV, JSON, Excel, HTML, XML, and many other file types, or loading data from a SQL or Google BigQuery database.</p>\n<p>You can check all the functions for loading DataFrames on this page of the Pandas <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/io.html\">documentation</a>.</p>\n<p>In this tip, I\u2019m going to focus on the 2 main ways I load data into Pandas DataFrames: reading from CSV or connecting directly to an SQL database.</p>\n<h4>Importing data from an csv\u00a0file:</h4>\n<p>To import data from a csv file use <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html#:~:text=If%20True%20and%20parse_dates%20is,parsing%20speed%20by%205-10x.\">pd.read_csv()</a>:</p>\n<p>If your file is in the same folder as your notebook, the easiest way to use pd.read_csv() is just to pass the filename directly to the function:</p>\n<pre>df = pd.read_csv (\u2018file_name.csv\u2019)</pre>\n<p>In case your file is not in the same folder as your notebook, you pass the file path (absolute or relative) to the function:</p>\n<pre>#Absolute path:<br>df = pd.read_csv(\u2018C:/path/to/you/file/file_name.csv\u2019)</pre>\n<pre>#Relative path:<br>df = pd.read_csv(\u2018\u2026/ file_name.csv\u2019)</pre>\n<p>When passing just the file name to the function, Pandas will infer most of the information it needs, like separators, columns data types, and indexes. But you can also use the function parameters to better deal with the data you\u2019re going to\u00a0import.</p>\n<p>There are a series of parameters you can pass to <strong>read_csv</strong>, but the ones I use the most\u00a0are:</p>\n<blockquote>\n<strong>header:</strong> Row number(s) to use as the column names, and the start of the data. The default behavior is to infer the column\u00a0names.</blockquote>\n<blockquote>\n<strong>sep: </strong>Delimiter to use, default is a\u00a0comma.</blockquote>\n<blockquote>\n<strong>index_col: </strong>Column(s) to use as the row labels of the DataFrame.</blockquote>\n<blockquote>\n<strong>dtype: </strong>Data type for data or columns, when you want to explicitly inform the dtype of columns when importing,<strong> E.g.</strong> {\u2018a\u2019: np.float64}.</blockquote>\n<blockquote>\n<strong>na_values: </strong>Additional strings to recognize as NA/NaN. Say you are importing a CSV where there are \u201cNone\u201d values, you can pass na_values=[\u2018None\u2019] and pandas will convert them to NaN when importing.</blockquote>\n<blockquote>\n<strong>parse_dates and infer_datetime_format: </strong>Useful when dealing with date columns. To keep this short I suggest looking at the <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html#:~:text=If%20True%20and%20parse_dates%20is,parsing%20speed%20by%205-10x.\">documentation</a> if you have to use these parameters.</blockquote>\n<h4><strong>Importing data from an SQL Database:</strong></h4>\n<p>The two main ways I import data from SQL databases are:</p>\n<p><strong>Importing from PostgreSQL databases:</strong></p>\n<p>Follow my guide for <a href=\"https://medium.com/@alestamm/importing-data-from-a-postgresql-database-to-a-pandas-dataframe-5f4bffcd8bb2\">importing data from a PostgreSQL database to a Pandas DataFrame</a>.</p>\n<p><strong>Importing data from SQLite databases:</strong></p>\n<p>To import data from an SQLite Database, I use a really helpful library called <strong>sqlite3</strong>(<a href=\"https://docs.python.org/3/library/sqlite3.html#module-sqlite3\">docs</a>).</p>\n<p>You can read the documentation and also use the guide I made for <a href=\"https://medium.com/@alestamm/importing-data-from-a-postgresql-database-to-a-pandas-dataframe-5f4bffcd8bb2\">importing data from PostgreSQL database </a>as a reference since the functions are very\u00a0similar.</p>\n<h4>Aside from importing data, you can also create DataFrames using these functions:</h4>\n<p>Using pd.DataFrame.from_dict() and passing a dictionary:</p>\n<pre>data = {\u2018col_1\u2019: [3, 2, 1, 0], \u2018col_2\u2019: [\u2018a\u2019, \u2018b\u2019, \u2018c\u2019, \u2018d\u2019]}<br>pd.DataFrame.from_dict(data)</pre>\n<p>If you are not running your notebook on the cloud (like on <a href=\"https://colab.research.google.com/notebooks/intro.ipynb?hl=pt_BR\">Google Colab</a>), you can read the data directly from your clipboard using <strong>pd.read_clipboard().</strong></p>\n<p>You can also create an DataFrame from a list or numpy array using <strong>pd.DataFrame():</strong></p>\n<pre>pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])</pre>\n<pre>or</pre>\n<pre>pd.DataFrame(list)</pre>\n<p>That\u2019s all for\u00a0today!</p>\n<p>Follow me to read my upcoming tips for working with\u00a0Pandas!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=2f1a3586e415\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["data-visualization","python","database","pandas","data-science"]},{"title":"Importing data from a PostgreSQL database to a Pandas DataFrame","pubDate":"2022-06-11 20:47:21","link":"https://medium.com/@alestamm/importing-data-from-a-postgresql-database-to-a-pandas-dataframe-5f4bffcd8bb2?source=rss-bdc9fb848ad5------2","guid":"https://medium.com/p/5f4bffcd8bb2","author":"Alexandre Stamm","thumbnail":"https://cdn-images-1.medium.com/max/1024/1*vE2qMiigkkMGdNf1vqNHbw.jpeg","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*vE2qMiigkkMGdNf1vqNHbw.jpeg\"><figcaption>Photo by <a href=\"https://unsplash.com/@purzlbaum\">Claudio Schwarz</a> on\u00a0<a href=\"https://unsplash.com/\">unsplash</a></figcaption></figure><p>In this article, we\u2019ll go over how to create a pandas DataFrame using a simple connection and query to fetch data from a PostgreSQL database that requires authentication.</p>\n<p>To work with PostgreSQL databases in Python I use <a href=\"https://pypi.org/project/psycopg2/\"><strong>psycopg2</strong></a>, which is the most popular PostgreSQL database adapter for\u00a0Python.</p>\n<p>The documentation for psycopg2 is found here: <a href=\"https://www.psycopg.org/docs/\">https://www.psycopg.org/docs/</a></p>\n<p>To use psycopg2, we first need to install\u00a0it:</p>\n<pre>pip install psycopg2</pre>\n<p>Then, we need to connect to the database using the connect() function\u00a0(<a href=\"https://www.psycopg.org/docs/connection.html\">docs</a>):</p>\n<pre>conn = psycopg2.connect(\u201cdbname=test user=postgres password=secret\u201d)</pre>\n<p>The basic connection parameters are:</p>\n<pre><em>\u00b7 dbname \u2014 the database name</em></pre>\n<pre><em>\u00b7 user \u2014 user name used to authenticate</em></pre>\n<pre><em>\u00b7 password \u2014 password used to authenticate</em></pre>\n<pre><em>\u00b7 host \u2014 database host address</em></pre>\n<pre><em>\u00b7 port \u2014 connection port number (defaults to 5432 if not provided)</em></pre>\n<p>After creating the connection, we need to open a cursor to perform database operations (<a href=\"https://www.psycopg.org/docs/connection.html#connection.cursor\">docs</a>):</p>\n<pre><strong>cur = conn.cursor()</strong></pre>\n<p>Then we need to execute a command using the cursor. We can do all sorts of commands (<strong>CREATE TABLE, INSERT INTO, SELECT\u2026</strong>). Here we are going to focus on using the cursor to fetch\u00a0data:</p>\n<pre><strong>cur.execute(\"SELECT * FROM test;\"):</strong> Execute the query;</pre>\n<pre><strong>cur.fetchone()</strong>: Fetch the next row of a query result set, returning a single tuple, or None when no more data is available;</pre>\n<pre><strong>cur.fetchmany([size=cursor.arraysize]):</strong> Fetch the next set of rows of a query result, returning a list of tuples.</pre>\n<pre><strong>cur.fetchall():</strong> Fetch all (remaining) rows of a query result, returning them as a list of tuples</pre>\n<p>If any changes were made to the database we need to\u00a0commit:</p>\n<pre>conn.commit()</pre>\n<p>When we are done, we need to close the communication with the database:</p>\n<pre>cur.close()</pre>\n<pre>conn.close()</pre>\n<p>Wrapping all this up in a function for convenience:</p>\n<p>First, let\u2019s say we want to connect to our work database and it requires authentication. To avoid writing our credentials directly in our notebook and exposing them if we need to upload our code into GitHub or share our notebook with a colleague after our analysis is done, we can create a dotenv file with our credentials, and load it into our connect function. Then we can add our dotenv file to gitignore and also directly share our notebook without having our credentials leaked.</p>\n<p>Here\u2019s how to do\u00a0it:</p>\n<p><strong>Create a\u00a0.env file and write our credentials on\u00a0it:</strong></p>\n<blockquote>Since I\u2019m using Linux, I use a simple \u201ctouch\u00a0.env\u201d command to create the file. On other OS, just create the file whichever way is easier for\u00a0you.</blockquote>\n<p>Then, open the file and write our credentials:</p>\n<pre>DB_USERNAME=name</pre>\n<pre>DB_PASSWORD=password</pre>\n<pre>DB_PATH=path_to_database(if it\u2019s on AWS for example, you can type in the address like: yourcompany-database-address.us-east.rds.amazonaws.com)</pre>\n<pre>DB_NAME=db_name</pre>\n<p>Save and close the\u00a0file.</p>\n<p>Then, we are going to use dotenv python library to load our credentials:</p>\n<pre>pip install python-dotenv<br>from dotenv import load_dotenv<br>load_dotenv() # take environment variables from .env.</pre>\n<p>Now, for the function I use to connect to the database:</p>\n<pre>import psycopg2</pre>\n<pre>import os</pre>\n<pre>import sys</pre>\n<pre>def connect():<br><br>   \u201c\u201d\u201d Connect to database \u201c\u201d\u201d</pre>\n<pre>   conn = None</pre>\n<pre>   try:</pre>\n<pre>      print(\u2018Connecting\u2026\u2019)</pre>\n<pre>      conn = psycopg2.connect(</pre>\n<pre>                   host=os.environ[\u2018DB_PATH\u2019],</pre>\n<pre>                   database=os.environ[\u2018DB_NAME\u2019],</pre>\n<pre>                   user=os.environ[\u2018DB_USERNAME\u2019],</pre>\n<pre>                   password=os.environ[\u2018DB_PASSWORD\u2019])</pre>\n<pre>    except (Exception, psycopg2.DatabaseError) as error:</pre>\n<pre>       print(error)</pre>\n<pre>       sys.exit(1)</pre>\n<pre>   print(\u201cAll good, Connection successful!\u201d)</pre>\n<pre>   return conn</pre>\n<p>And now we write a function to create a pandas DataFrame using a SELECT\u00a0query:</p>\n<pre>def sql_to_dataframe(conn, query, column_names):</pre>\n<pre>   \u201c\u201d\u201d <br>   Import data from a PostgreSQL database using a SELECT query <br>   \u201c\u201d\u201d</pre>\n<pre>   cursor = conn.cursor()</pre>\n<pre>   try:</pre>\n<pre>      cursor.execute(query)</pre>\n<pre>   except (Exception, psycopg2.DatabaseError) as error:</pre>\n<pre>      print(\u201cError: %s\u201d % error)</pre>\n<pre>   cursor.close()</pre>\n<pre>   return 1</pre>\n<pre>   # The execute returns a list of tupples:</pre>\n<pre>   tupples_list = cursor.fetchall()</pre>\n<pre>   cursor.close()</pre>\n<pre>   # Now we need to transform the list into a pandas DataFrame:</pre>\n<pre>   df = pd.DataFrame(tupples_list, columns=column_names)</pre>\n<pre>   return df</pre>\n<p>For better convenience, I also recommend writing our functions into a\u00a0.py file so we can reuse it whenever we\u00a0want.</p>\n<p>Now that we have everything we need, let\u2019s finally put it all together and load our DataFrame from our database. Here\u2019s the whole\u00a0code:</p>\n<pre>#imports<br>import pandas as pd<br>import numpy as np<br>import os<br>import psycopg2<br>from dotenv import load_dotenv<br>from functions import sql_to_dataframe, connect<br>load_dotenv()</pre>\n<pre><em>#creating a query variable to store our query to pass into the function</em></pre>\n<pre>query = \u201c\u201d\u201d SELECT column1, <br>                   column2, <br>                   column3 <br>            FROM database<br>        \u201d\u201d\u201d<br><em>#creating a list with columns names to pass into the function</em></pre>\n<pre>column_names = [\u2018column1\u2019,\u2018column2\u2019, \u2018column3\u2019]</pre>\n<pre><em>#opening the connection</em></pre>\n<pre>conn = connect()</pre>\n<pre><em>#loading our dataframe</em></pre>\n<pre>df = sql_to_dataframe(conn, query, column_names)</pre>\n<pre><em>#closing the connection</em></pre>\n<pre>conn.close()</pre>\n<pre># Let\u2019s see if we loaded the df successfully</pre>\n<pre>df.head()</pre>\n<p>Finally we\u2019re\u00a0done!</p>\n<p>In this tutorial, you learned how to import data from a PostgreSQL database to a pandas DataFrame, while also keeping your credentials safe and writing a useful function that you can reuse in the\u00a0future!</p>\n<p>Thanks for\u00a0reading!</p>\n<p>Sources:</p>\n<p><a href=\"https://www.psycopg.org/docs/connection.html#connection.cursor\">https://www.psycopg.org/docs/connection.html</a></p>\n<p><a href=\"https://www.postgresqltutorial.com/postgresql-python/connect/\">https://www.postgresqltutorial.com/postgresql-python/connect/</a></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=5f4bffcd8bb2\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*vE2qMiigkkMGdNf1vqNHbw.jpeg\"><figcaption>Photo by <a href=\"https://unsplash.com/@purzlbaum\">Claudio Schwarz</a> on\u00a0<a href=\"https://unsplash.com/\">unsplash</a></figcaption></figure><p>In this article, we\u2019ll go over how to create a pandas DataFrame using a simple connection and query to fetch data from a PostgreSQL database that requires authentication.</p>\n<p>To work with PostgreSQL databases in Python I use <a href=\"https://pypi.org/project/psycopg2/\"><strong>psycopg2</strong></a>, which is the most popular PostgreSQL database adapter for\u00a0Python.</p>\n<p>The documentation for psycopg2 is found here: <a href=\"https://www.psycopg.org/docs/\">https://www.psycopg.org/docs/</a></p>\n<p>To use psycopg2, we first need to install\u00a0it:</p>\n<pre>pip install psycopg2</pre>\n<p>Then, we need to connect to the database using the connect() function\u00a0(<a href=\"https://www.psycopg.org/docs/connection.html\">docs</a>):</p>\n<pre>conn = psycopg2.connect(\u201cdbname=test user=postgres password=secret\u201d)</pre>\n<p>The basic connection parameters are:</p>\n<pre><em>\u00b7 dbname \u2014 the database name</em></pre>\n<pre><em>\u00b7 user \u2014 user name used to authenticate</em></pre>\n<pre><em>\u00b7 password \u2014 password used to authenticate</em></pre>\n<pre><em>\u00b7 host \u2014 database host address</em></pre>\n<pre><em>\u00b7 port \u2014 connection port number (defaults to 5432 if not provided)</em></pre>\n<p>After creating the connection, we need to open a cursor to perform database operations (<a href=\"https://www.psycopg.org/docs/connection.html#connection.cursor\">docs</a>):</p>\n<pre><strong>cur = conn.cursor()</strong></pre>\n<p>Then we need to execute a command using the cursor. We can do all sorts of commands (<strong>CREATE TABLE, INSERT INTO, SELECT\u2026</strong>). Here we are going to focus on using the cursor to fetch\u00a0data:</p>\n<pre><strong>cur.execute(\"SELECT * FROM test;\"):</strong> Execute the query;</pre>\n<pre><strong>cur.fetchone()</strong>: Fetch the next row of a query result set, returning a single tuple, or None when no more data is available;</pre>\n<pre><strong>cur.fetchmany([size=cursor.arraysize]):</strong> Fetch the next set of rows of a query result, returning a list of tuples.</pre>\n<pre><strong>cur.fetchall():</strong> Fetch all (remaining) rows of a query result, returning them as a list of tuples</pre>\n<p>If any changes were made to the database we need to\u00a0commit:</p>\n<pre>conn.commit()</pre>\n<p>When we are done, we need to close the communication with the database:</p>\n<pre>cur.close()</pre>\n<pre>conn.close()</pre>\n<p>Wrapping all this up in a function for convenience:</p>\n<p>First, let\u2019s say we want to connect to our work database and it requires authentication. To avoid writing our credentials directly in our notebook and exposing them if we need to upload our code into GitHub or share our notebook with a colleague after our analysis is done, we can create a dotenv file with our credentials, and load it into our connect function. Then we can add our dotenv file to gitignore and also directly share our notebook without having our credentials leaked.</p>\n<p>Here\u2019s how to do\u00a0it:</p>\n<p><strong>Create a\u00a0.env file and write our credentials on\u00a0it:</strong></p>\n<blockquote>Since I\u2019m using Linux, I use a simple \u201ctouch\u00a0.env\u201d command to create the file. On other OS, just create the file whichever way is easier for\u00a0you.</blockquote>\n<p>Then, open the file and write our credentials:</p>\n<pre>DB_USERNAME=name</pre>\n<pre>DB_PASSWORD=password</pre>\n<pre>DB_PATH=path_to_database(if it\u2019s on AWS for example, you can type in the address like: yourcompany-database-address.us-east.rds.amazonaws.com)</pre>\n<pre>DB_NAME=db_name</pre>\n<p>Save and close the\u00a0file.</p>\n<p>Then, we are going to use dotenv python library to load our credentials:</p>\n<pre>pip install python-dotenv<br>from dotenv import load_dotenv<br>load_dotenv() # take environment variables from .env.</pre>\n<p>Now, for the function I use to connect to the database:</p>\n<pre>import psycopg2</pre>\n<pre>import os</pre>\n<pre>import sys</pre>\n<pre>def connect():<br><br>   \u201c\u201d\u201d Connect to database \u201c\u201d\u201d</pre>\n<pre>   conn = None</pre>\n<pre>   try:</pre>\n<pre>      print(\u2018Connecting\u2026\u2019)</pre>\n<pre>      conn = psycopg2.connect(</pre>\n<pre>                   host=os.environ[\u2018DB_PATH\u2019],</pre>\n<pre>                   database=os.environ[\u2018DB_NAME\u2019],</pre>\n<pre>                   user=os.environ[\u2018DB_USERNAME\u2019],</pre>\n<pre>                   password=os.environ[\u2018DB_PASSWORD\u2019])</pre>\n<pre>    except (Exception, psycopg2.DatabaseError) as error:</pre>\n<pre>       print(error)</pre>\n<pre>       sys.exit(1)</pre>\n<pre>   print(\u201cAll good, Connection successful!\u201d)</pre>\n<pre>   return conn</pre>\n<p>And now we write a function to create a pandas DataFrame using a SELECT\u00a0query:</p>\n<pre>def sql_to_dataframe(conn, query, column_names):</pre>\n<pre>   \u201c\u201d\u201d <br>   Import data from a PostgreSQL database using a SELECT query <br>   \u201c\u201d\u201d</pre>\n<pre>   cursor = conn.cursor()</pre>\n<pre>   try:</pre>\n<pre>      cursor.execute(query)</pre>\n<pre>   except (Exception, psycopg2.DatabaseError) as error:</pre>\n<pre>      print(\u201cError: %s\u201d % error)</pre>\n<pre>   cursor.close()</pre>\n<pre>   return 1</pre>\n<pre>   # The execute returns a list of tupples:</pre>\n<pre>   tupples_list = cursor.fetchall()</pre>\n<pre>   cursor.close()</pre>\n<pre>   # Now we need to transform the list into a pandas DataFrame:</pre>\n<pre>   df = pd.DataFrame(tupples_list, columns=column_names)</pre>\n<pre>   return df</pre>\n<p>For better convenience, I also recommend writing our functions into a\u00a0.py file so we can reuse it whenever we\u00a0want.</p>\n<p>Now that we have everything we need, let\u2019s finally put it all together and load our DataFrame from our database. Here\u2019s the whole\u00a0code:</p>\n<pre>#imports<br>import pandas as pd<br>import numpy as np<br>import os<br>import psycopg2<br>from dotenv import load_dotenv<br>from functions import sql_to_dataframe, connect<br>load_dotenv()</pre>\n<pre><em>#creating a query variable to store our query to pass into the function</em></pre>\n<pre>query = \u201c\u201d\u201d SELECT column1, <br>                   column2, <br>                   column3 <br>            FROM database<br>        \u201d\u201d\u201d<br><em>#creating a list with columns names to pass into the function</em></pre>\n<pre>column_names = [\u2018column1\u2019,\u2018column2\u2019, \u2018column3\u2019]</pre>\n<pre><em>#opening the connection</em></pre>\n<pre>conn = connect()</pre>\n<pre><em>#loading our dataframe</em></pre>\n<pre>df = sql_to_dataframe(conn, query, column_names)</pre>\n<pre><em>#closing the connection</em></pre>\n<pre>conn.close()</pre>\n<pre># Let\u2019s see if we loaded the df successfully</pre>\n<pre>df.head()</pre>\n<p>Finally we\u2019re\u00a0done!</p>\n<p>In this tutorial, you learned how to import data from a PostgreSQL database to a pandas DataFrame, while also keeping your credentials safe and writing a useful function that you can reuse in the\u00a0future!</p>\n<p>Thanks for\u00a0reading!</p>\n<p>Sources:</p>\n<p><a href=\"https://www.psycopg.org/docs/connection.html#connection.cursor\">https://www.psycopg.org/docs/connection.html</a></p>\n<p><a href=\"https://www.postgresqltutorial.com/postgresql-python/connect/\">https://www.postgresqltutorial.com/postgresql-python/connect/</a></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=5f4bffcd8bb2\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["pandas","postgres","data-science","pandas-dataframe","python"]}]}